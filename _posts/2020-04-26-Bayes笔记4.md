---
layout:     post                    # ‰ΩøÁî®ÁöÑÂ∏ÉÂ±ÄÔºà‰∏çÈúÄË¶ÅÊîπÔºâ
title:      BayesÁ¨îËÆ∞Âõõ(Prior)               # Ê†áÈ¢ò
subtitle:   Series notes of SDSC6003 course. #ÂâØÊ†áÈ¢ò
date:       2020-04-26              # Êó∂Èó¥
author:     Qingyu                      # ‰ΩúËÄÖ
header-img: img/post-bg-unix-linux.jpg  #ËøôÁØáÊñáÁ´†Ê†áÈ¢òËÉåÊôØÂõæÁâá
catalog: true                       # ÊòØÂê¶ÂΩíÊ°£
tags:                               #Ê†áÁ≠æ
    - Bayes Theory
---

**Prior distributions**

In practice, the statistician often assumes that the <u>prior distribution belongs to a reasonable parametric family of distributions</u>, and <u>then determine its parameters</u> by asking the decision maker some questions.

- Example: suppose that ùúÉ is the probability of success in a binomial distribution. Then, a beta distribution $g(\theta)=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \theta^{a-1}(1-\theta)^{b-1}, 0<\theta<1,$ can be used as a prior distribution.
  We were told that the median of $\theta$ is 0.5  (it is equally likely for } $\theta$ to be greater than 0.5 or less than 0.5). We were also told that there is a $95 \%$ chance that $\theta$ is greater than 0.1( the 0.05 quantile of $\theta$ is 0.1). The value of $(a, b)$ that gives a 0.05 quatitle of 0.1 and a median of 0.5 needs to be determined numerically. For example, we can use Matlab‚Äôs fsolve function.

- Example (continued): Now, suppose we were told that the mean and variance of
  $\theta$ are $E(\theta)=0.5$ and $\operatorname{var}(\theta)=0.1$. Then, we have 

  $\frac{a}{a+b}=E(\theta)=0.5$

  $\frac{a b}{(a+b)^{2}(a+b+1)}=\operatorname{var}(\theta)=0.1$
  Then, we obtain from the first equation that $b /(a+b)=1-0.5=0.5$ Plugging this and the first equation into the second gives $0.5^{2} /(a+b+1)=$ $0.1,$ which yields $a+b=1.5 .$ This result and the first equation give $a=0.5 \times 1.5=0.75 .$ Hence, $b=0.75$ also.

We shall address specification of prior distributions for two cases:

- No prior information is available or when prior knowledge is of little significance compared to information from the data: Some prior distributions are designed to be uninformative and these are commonly called **noninformative priors**.

  Nowadays, they are widely viewed as default choices that can be used when prior knowledge is weak, which makes elicitation of a subjective prior not worth the effort, or when elicitation of a subjective prior is deemed too much work.

  Inference based on these priors can also <u>be used as a reference point for comparison</u> when there is strong prior information to see how strongly the results are influenced by the prior distribution.

  In cases where the dataset is large, the impact of the prior distribution on Bayesian inference is negligible and a noninformative prior would be a convenient choice.

- Choice of <u>prior for analytical convenience in deriving the posterior distribution</u>, called **natural conjugate priors**.In this case, the prior distribution is assumed to be from a parametric family, and its parameters are chosen so that there is a match between elicited summary measures of the decision maker‚Äôs prior distribution and the corresponding summary measures of the natural conjugate prior.

  Methods for formal elicitation of summary measures of prior distributions is an intricate subject that involves asking a series of carefully designed questions. Hence, we shall not discuss this subject further.



**The Bayes-Laplace method**

This method recommends to use a discrete <u>uniform distribution</u> on the parameter space $\Theta$ when $\Theta$ has a finite/countable number rements, or a continuous uniform distribution on $\Theta$ when $\Theta$ has an uncountable number of elements.

- For example, when $\Theta=\left\{\theta_{1}, \ldots, \theta_{m}\right\},$ then this method recommends the prior PMF $g\left(\theta_{i}\right)=1 / m$ for all $i=1, \ldots, m .$

When $\Theta$ is countably infinite or when $\Theta$ is uncountable and unbounded, then the use of this method leads to an <u>improper prior distribution</u> $g(\theta) \propto 1 \forall \theta \in \Theta($ PMF that does not sum to one or PDF that does not istegrate to one). However, in many cases, use of these improper pricion distribution <u>leads to a proper posterior distribution</u> $g(\theta | x) \propto L(\theta)$ since the likelihood integrates to a finite number.

A criticism of this method is that the continuous uniform distribution is not invariant to one-one nonlinear transformations.

- Let $\psi=\xi(\theta)$ be a one-one transformation of a real valued parameter $\theta$. Then, if the prior for $\theta$ is $g(\theta)$, the prior for $\psi$ is $h(\psi)=g\left[\xi^{-1}(\psi)\right]\left|\frac{d \theta}{d \psi}\right|$

  For nonlinear one-one transformations, $h(\psi)$ is not a uniform PDF when $g(\theta)$ is a uniform PDF since $\left|\frac{d \theta}{d \psi}\right|$ is not a constant function.

it is indeed a noninformative prior.



**Jeffrey‚Äôs prior**

A class of priors that is <u>invariant under one-to-one transformations</u> is the Jeffrey‚Äôs prior, which is based on Fisher‚Äôs information:

$I(\theta)=E\left[\left(\frac{\partial \ln f(X | \theta)}{\partial \theta}\right)^{2} | \theta\right]=-E\left[\frac{\partial^{2} \ln f(X | \theta)}{\partial \theta^{2}} | \theta\right]$

Jeffrey's prior is defined as $g(\theta) \propto \sqrt{I(\theta)}$. Suppose $\psi=\xi(\theta) .$ Then, a change of variables give the prior distribution for $\psi:$

$h(\psi) \propto g(\theta)\left|\frac{d \theta}{d \psi}\right|=\sqrt{E\left[\left(\frac{\partial \ln f(X | \theta)}{\partial \theta}\right)^{2} | \theta\right]\left(\frac{d \theta}{d \psi}\right)^{2}}=\sqrt{E\left[\left(\frac{\partial \ln f(X | \theta)}{\partial \theta} \frac{d \theta}{d \psi}\right)^{2} | \theta\right]}=\sqrt{E\left[\left(\frac{\partial \ln f\left(X | \xi^{-1}(\psi)\right)}{\partial \psi}\right)^{2} | \psi\right]}$

Thus, we see that $h(\psi) \propto \sqrt{I(\psi)}$, which is precisely Jeffrey's prior for $\psi$ if we had parameterize $f$ in terms of $\psi .$

Jeffrey‚Äôs prior can depend on how the experiment is performed to obtain data to estimate a parameter.

Jeffrey‚Äôs prior <u>for location parameter</u>:

- Consider the family of PDF's defined over $\mathbb{R}$ with a location parameter $\theta:$ $\{f(x | \theta)=e(x-\theta): \theta \in \Theta \subset \mathbb{R}\} .$ Note that if $X$ has PDF $e(x-\theta)$ conditioned on $\theta,$ then $Z=X-\theta$ has PDF $e(z)$ conditioned on $\theta .$ Thus, $Z$ and $\theta$ are independent.We have
  $I(\theta)=E\left[\left(\frac{\partial \ln f(x | \theta)}{\partial \theta}\right)^{2} | \theta\right]=E\left[\left(-\frac{e^{\prime}(X-\theta)}{e(X-\theta)}\right)^{2} | \theta\right]=E\left[\left(\frac{e^{\prime}(Z)}{e(Z)}\right)^{2} | \theta\right]= E\left[\left(\frac{e^{\prime}(Z)}{e(Z)}\right)^{2}\right]=$ constant
  Here, we assume that $E\left[\left(\frac{e^{\prime}(Z)}{e(Z)}\right)^{2}\right]=\int_{\mathbb{R}} \frac{\left(e^{\prime}(Z)\right)^{2}}{e(Z)} d Z$ exists. Thus, Jeffrey's prior for $\theta$ is a uniform prior over $\Theta,$ which will be improper if $\Theta$ is unbounded.

Jeffrey‚Äôs prior <u>for scale parameter</u>:

- Consider the family of PDF's with a scale parameter $\theta:\left\{f(x | \theta)=\theta^{-1} e(x / \theta): \theta \in\right. (0, \infty)\}$. Note that if $X$ has PDF $\theta^{-1} e(x / \theta)$ conditioned on $\theta,$ then $Z=X / \theta$ has PDF $e(Z) $ conditioned on $\theta$. Thus, $Z$ and $\theta$ are independent. We have
  $I(\theta)=E\left[\left(\frac{\partial \ln f(X | \theta)}{\partial \theta}\right)^{2} | \theta\right]=E\left[\left(-\theta^{-1}-X \theta^{-2} \frac{e^{\prime}(X / \theta)}{e(X / \theta)}\right)^{2} | \theta\right]$
  $=E\left[\left(-\theta^{-1}-\theta^{-1} Z \frac{e^{\prime}(Z)}{e(Z)}\right)^{2} | \theta\right]=\theta^{-2} E\left[\left(1+Z \frac{e^{\prime}(Z)}{e(Z)}\right)^{2} | \theta\right]$
  $=\theta^{-2} E\left[\left(1+Z \frac{e^{\prime}(Z)}{e(Z)}\right)^{2}\right] \propto \theta^{-2}$
  It follows that Jeffrey's prior for $\theta$ is $g(\theta) \propto \theta^{-1} I(\theta>0)$. This prior is invariant under positive power transformations, i.e., if $\psi=\theta^{r}, r>0,$ then the prior for $\psi$ is $h(\psi) \propto \psi^{-1} I(\psi>0)$ by a change of variables.

However, it is often found that the prior has undesirable effects on the posterior distribution. Thus, it is recommended to 

1. derive the Jeffrey‚Äôs prior for each parameter assuming that the rest of the parameters are fixed 
2. assume that the parameters are a priori independent, and take the product of the priors as the joint prior distribution for the parameters.



**Maximum entropy methods**

The <u>entropy of a distribution</u> $g(\theta), \theta \in \Theta$ is the expected value $E[-\ln (g(\theta))]$

In the case where $\Theta=\left\{\theta_{1}, \dots, \theta_{m}\right\}$ is finite, the maximum entropy distribution is the discrete uniform distribution.

When $\Theta$ is a bounded interval on the real line, it can be shown that maximizing the entropy subject to $E\left(m_{j}(\theta)\right)=u_{j}, j=1, \ldots, l$ yields $g(\theta) \propto \exp \left[\sum_{j=1}^{l} \lambda_{j} m_{j}(\theta)\right]$ Thus, if there is no constraint, i.e., $l=0,$ the maximum entropy distribution is the uniform distribution on $\Theta .$



**Natural conjugate priors**

Derivation of the posterior distribution becomes straightforward if we use a family of prior distributions $\mathcal{H}=\left\{g_{a}(\theta): a \in \mathcal{A}\right\},$ where $\mathcal{A}$ is a set of values for the hyperparameters $a$ such that $\mathcal{H}$ is closed under sampling from $\mathcal{M}=\{f(x | \theta): \theta \in \Theta\}$. This means that $g(\theta) \in \mathcal{H} \Rightarrow g(\theta | x) \propto f(x | \theta) g(\theta) \in \mathcal{H}$ if $f(x | \theta) \in \mathcal{M}$. We <u>call $\mathcal{H}$ a natural conjugate family for $\mathcal{M}$.</u>

Most commonly, we find a natural conjugate family in the following situation: $L(\theta) \propto f(x| \theta)$ is proportional to a member of $\mathcal{H}$ and $\mathcal{H}$ is closed under product. i.e. if $g(\theta) \in \mathcal{H}$ and $g_{b}(\theta) \in \mathcal{H},$ then $g_{a}(\theta) g_{b}(\theta) \propto g_{c}(\theta) \in \mathcal{H}$

- Example: A natural conjugate family for the binomial sampling model $f(x | \theta) \propto \theta^{x}(1-\theta)^{n-x}$ is the family of beta distributions $ g(\theta)=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \theta^{a-1}(1-\theta)^{b-1} I(0<\theta<1)$ where $a>0$ and $b>0 .$ This family of two-parameter distributions is highly flexible . The posterior under this prior is $g(\theta | x) \propto \theta^{a+x-1}(1-\theta)^{b+n-x-1}$ which is a beta distribution with parameters $a+x$ and $b+n-x$. The posterior mean of $\theta$ is $E(\theta | x)=\frac{a+x}{a+x+b+n-x}=\frac{a+x}{a+b+n}$

