---
layout:     post                    # 使用的布局（不需要改）
title:      Bayes笔记二(Frequentist、bootstrap、RV generation)               # 标题
subtitle:   Series notes of SDSC6003 course. #副标题
date:       2020-04-24              # 时间
author:     Qingyu                      # 作者
header-img: img/post-bg-unix-linux.jpg  #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Bayes Theory
---

**Frequentist inference**

Frequentist methods include methods to determine whether the data $x$ are compatible with the model $\mathcal{M}$ and what conclusions can be drawn about a parameter $\theta$.

- Frequentist methods are evaluated under the principle of repeated sampling.
- **Point estimation** is a fundamental question in frequentist inference.
  - Let the data $X_{1}, \ldots, X_{n}$ be iid with CDF $F$ and $\theta=P(F)$ is a scalar/vector of interest. Then, a <u>point estimate</u> of $\theta,$ denoted by $\hat{\theta},$ is a single "best guess" of $\theta$ based on the data, i.e., $\hat{\theta}=T(X) .$
  - The <u>bias of the estimator</u> $\hat{\theta}$ is bias $(\hat{\theta})=E_{F}(\hat{\theta})-\theta .$ We write $E_{F}$ instead
    of $E$ to emphasize that the expectation is with respect to $F .$
  - <u>Unbiasedness</u>: $E_{F}(T(X))=\int_{X} T(x) F(d x)=\theta$  $  \forall \theta \in \Theta$. This says that
    under repeated sampling, the estimator will have average value equal to $\theta$
    whatever that value is.
  - The <u>mean squared error</u> $(\mathrm{MSE})$ of $\hat{\theta}$ is $E_{F}\left[(\hat{\theta}-\theta)^{2}\right]=\operatorname{bias}^{2}(\hat{\theta})+\operatorname{var}(\hat{\theta})$
  - A point estimator $\hat{\theta}$ of $\theta$ is <u>consistent</u> if $\hat{\theta} \rightarrow^{P} \theta$ as $n \rightarrow \infty$
- A $1-\alpha$ **confidence set** for a parameter $\theta$ is a set $C$ that depends on the data but not $\theta$ such that $\mathbb{P}(\theta \in C) \geq 1-\alpha$ for all $F \in \mathcal{M}$. Usually, $\theta$ is a scalar and the set is constrained to be an interval.
  - Interpretation: under repeated sampling, at least $1-\alpha$ of the constructed intervals will contain $\theta$. 

**empirical distribution function**

Let $X_{1}, \dots, X_{n} \sim F$. We can estimate with the empirical distribution function, which is defined as follows.

- Definition 1.21: The empirical distribution function $\widehat{F}_{n}$ is the CDF that puts probability mass 1 $/ n$ at each data point $X_{i}$. Formally,
  $\hat{F}_{n}(x)=n^{-1} \sum_{i=1}^{n} I\left(X_{i} \leq x\right)$
- $\mathrm{A} 1-\alpha$ confidence band for $F$ is given as follows: Define $L(x)= \max \left\{\hat{F}_{n}(x)-\varepsilon_{n}, 0\right\}$ and $U(x)=\min \left\{\widehat{F}_{n}(x)+\varepsilon_{n}, 1\right\},$ where $\varepsilon_{n}= \sqrt{\log (2 / \alpha) /(2 n)}$. Then, $\mathbb{P}[L(x)\leq F(x) \leq U(x) \text { for all } x] \geq 1-\alpha$.
- Definition 1.22: The **plug-in estimator** of $\theta=P(F)$ is defined by $\hat{\theta}_{n}= P\left(\hat{F}_{n}\right)=T(X) .$ In other words, just replace $F$ with $\hat{F}_{n}$.
  - Example: The plug-in estimator for a linear functional $P(F)= \int_{\mathbb{R}} r(x) d F(x)$ is $P\left(\widehat{F}_{n}\right)=\int_{\mathbb{R}} r(x) d \widehat{F}_{n}(x)=n^{-1} \sum_{i=1}^{n} r\left(X_{i}\right)=T(X)$



**Bootstrap**

The bootstrap is a method for <u>estimating standard errors</u> and c<u>omputing confidence intervals</u>. The bootstrap method consists of **two basic steps**:
Step 1: Estimate var $_{X \sim F}(T(X))$ with var $_{X \sim \hat{F}_{n}}(T(X))$
Step 2: Approximate var $_{X \sim \hat{F}_{n}}(T(X))$ using Monte Carlo simulation.

- Bootstrap variance estimation

  1. Draw $X_{1}^{*}, \ldots, X_{n}^{*} \sim \widehat{F}_{n}$
  2. Compute $\hat{\theta}_{n}^{*}=T\left(X^{*}\right),$ where $X^{*}=\left(X_{1}^{*}, \ldots, X_{n}^{*}\right)$
  3. Repeat steps 1 and $2, B$ times, to get $\hat{\theta}_{n, 1}^{*}, \ldots, \hat{\theta}_{n, B}^{*}$
  4. The bootstrap variance estimate is $v_{\text {boot }}=B^{-1} \sum_{b=1}^{B}\left(\hat{\theta}_{n, b}^{*}-B^{-1} \sum_{r=1}^{B} \hat{\theta}_{n, r}^{*}\right)^{2}$

  Usually, $B$ is large, which makes the second source of error negligible.

- Bootstrap confidence intervals

  - Denote $\hat{\theta}_{n, \alpha}^{*}$ as the $\alpha$ quantile of the bootstrap samples $\hat{\theta}_{n, 1}^{*}, \ldots, \hat{\theta}_{n, B}^{*}$

  - **Percentile intervals**. The bootstrap percentile interval is defined by
    $C_{n}=\left[\hat{\theta}_{n, \frac{\alpha}{2}}^{*}, \hat{\theta}_{n, 1-\frac{\alpha}{2}}^{*}\right]$

  - **Pivotal intervals**. the $1-\alpha$ bootstrap pivotal confidence interval is
    $C_{n}=\left[2 \hat{\theta}_{n}-\hat{\theta}_{n, 1-\frac{\alpha}{2}}^{*}, 2 \hat{\theta}_{n}-\hat{\theta}_{n, \frac{\alpha}{2}}^{*}\right]$

  - Under some conditions, $\mathbb{P}\left(\theta \in C_{n}\right) \rightarrow 1-\alpha$ as $n \rightarrow \infty$

    

**Random variate generation**

- <u>Inverse transform method</u>: Suppose that $F$ is a CDF. If $U$ is uniformly distributed on $[0,1],$ then $X=F^{-1}(U) \sim F,$ where $F^{-1}(u)= \inf \{x: F(x) \geq u\}, 0<u<1 $

  - **Theorem 1.18**$: X=F^{-1}(U)$ is a random variable with CDF $F$

  Example: Suppose we want to <u>generate exponential random variables</u>:

  - $F(x)=\int_{0}^{x} \lambda e^{-\lambda} d v=1-e^{-\lambda x}$
  - $F(x)=u=1-e^{-\lambda x} \Rightarrow x=F^{-1}(u)=-\frac{1}{\lambda} \ln (1-u)$

  - Thus, if $U_{1}, \ldots, U_{n}$ are independent uniform random variables on $[0,1]$, $X_{1}=-\frac{1}{\lambda} \ln \left(1-U_{1}\right), \ldots, X_{n}=-\frac{1}{\lambda} \ln \left(1-U_{n}\right)$ are independent and identical exponential random variables with rate $\lambda$.

- <u>Rejection sampling method</u>: Rejection sampling method: Let $X$ be a random variable with PDF $c f,$ where $c$ is a (possibly unknown) scalar, and let $g$ be a density function such that $f(x) \leq M g(x)$ for all $x$ for some constant $M$. It is assumed that we can generate a random sample easily from $g .$ The rejection sampling method consists of the following steps:

  1. Generate $Y \sim g$ and $U \sim$ unif( $[0,1]$ ) independently.
  2. If $U \leq f(Y) /[M g(Y)],$ set $X=Y .$ Otherwise, return to $1 .$

  - **Theorem 1.19**: The PDF of $X$ is $c f$.

  Example: Derive an acceptance sampling scheme for generating from a Gamma(k, $\theta$ ) distribution using the exponential PDF as $g$.

  - Note that $f(x)=\frac{1}{\Gamma(k) \theta^{k}} x^{k-1} e^{-x / \theta}, x \geq 0$ and $g(x)=\lambda e^{-\lambda x}, x \geq 0$
  - Define $r(x)=\frac{f(x)}{g(x)}=\frac{1}{\lambda \Gamma(k) \theta^{k}} x^{k-1} e^{(\lambda-1 / \theta) x}, x \geq 0$
  - We find that $r^{\prime}(x)=\frac{1}{\lambda \Gamma(k) \theta^{k}} x^{k-2} e^{(\lambda-1 / \theta) x}\left[k-1+x\left(\lambda-\frac{1}{\theta}\right)\right]$
  - Thus, if $\lambda<1 / \theta, r(x)$ achieves a maximum at $x^{*}=\frac{k-1}{\frac{1}{\theta}-\lambda}$ and $r\left(x^{*}\right)=\frac{(k-1)^{k-1}}{\lambda \Gamma(k) \theta^{k}\left(\frac{1}{\theta}-\lambda\right)^{k-1}} e^{-(k-1)}=M$
  - We can choose $\lambda$ to maximize acceptance rate, i.e., minimize $M .$ This gives $\lambda=$
    $1 /(k \theta) .$

