---
layout:     post                    # 使用的布局（不需要改）
title:      Bayes笔记八(Bayesian Regression)               # 标题
subtitle:   Series notes of SDSC6003 course. #副标题
date:       2020-05-05              # 时间
author:     Qingyu                      # 作者
header-img: img/post-bg-unix-linux.jpg  #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Bayes Theory

---

##### Linear regression

In linear regression, we are interested to predict $Y$ as function of $x \in x$ c $\mathbb{R}^{d}$ based on the linear model $Y(x)=m(x) \beta+\varepsilon$
where $m(x)=\left(m_{1}(x), \ldots, m_{p}(x)\right),$ each $m_{i}(x)$ is a <u>real-valued function</u> of $x, \beta$ is a $p \times 1$ column vector of regression coefficients, and $\varepsilon$ represents random error with mean zero and variance $\sigma^{2}$. We have data $\left(x_{1}, Y_{1}\right), \ldots,\left(x_{n}, Y_{n}\right),$ where $x_{i}=\left(x_{i 1}, \ldots, x_{i d}\right) .$ Note that $x_{1}, \ldots, x_{n}$ are
treated as fixed quantities. The linear regression model for the data is $Y_{i}=m\left(x_{i}\right) \beta+\varepsilon_{i}, i=1, \ldots, n$
where $\varepsilon_{i}, i=1, \ldots, n$ are iid with mean zero and variance $\sigma^{2}$

We can write the model for the data as
$Y=M \beta+\varepsilon$
where $Y=\left(Y_{1}, \ldots, Y_{n}\right)^{T}, \varepsilon=\left(\varepsilon_{1}, \ldots, \varepsilon_{n}\right)^{T}$, and $M=\left(m\left(x_{1}\right)^{T}, \ldots, m\left(x_{n}\right)^{T}\right)^{T}$ is assumed
to be a matrix with full column rank (all its columns are linearly independent).

<u>Suppose we want to estimate $\beta .$</u> Then, an intuitively reasonable way to proceed is to minimize the sum of squared error $\mathrm{E}(\beta)=(Y-M \beta)^{T}(Y-M \beta)=\|Y-M \beta\|_{2}^{2}$. This leads to
$\nabla \mathrm{E}(\beta)=2 M^{T} M \beta-2 M^{T} Y$ (gradient), and $\mathcal{H} \mathrm{E}(\beta)=2 M^{T} M(\text { Hessian })$
since $M$ has full column rank, $M^{T} M$ is positive definite for all $\beta .$ Thus, there is a unique stationary point $\hat{\beta}=\left(M^{T} M\right)^{-1} M^{T} Y .$ since $\mathcal{H} \mathrm{E}(\beta)$ is positive definite for all $\beta,$ we conclude that $\mathrm{E}(\beta)$ is a strictly convex function and $\hat{\beta}$ is the global minimum point.

##### Gauss-Markov theorem

Consider the linear regression problem: $Y_{i}=$ $m\left(x_{i}\right) \beta+\varepsilon_{i}, i=1, \ldots, n,$ where $\varepsilon_{i}, i=1, \ldots, n$ are iid with mean zero
and variance $\sigma^{2}$

1. Among all estimators of $\beta$ of the form $A Y+B$ (linear in $Y$ ) with fixed matrices $A$ and $B$ that are unbiased, the least squares estimator $\hat{\beta}=C Y,$ where $C=\left(M^{T} M\right)^{-1} M^{T},$ has the lowest variance.
2. Among all estimators of $a^{T} \beta$ that are linear in $Y$ and unbiased, the estimator $a^{T} \hat{\beta}$ has the lowest variance.

**Normally distributed errors**

Consider the linear regression problem: $Y_{i}=m\left(x_{i}\right) \beta+\varepsilon_{i}, i=1, \ldots, n$ where $\varepsilon_{i}, i=1, \ldots, n$ are iid normal with mean zero and variance $\sigma^{2} .$ In this case, the likelihood is $L(\beta)=\left(\sigma^{2}\right)^{-n / 2} \exp \left\{-\frac{1}{2 \sigma^{2}}(Y-M \beta)^{T}(Y-M \beta)\right\}$

Thus, $\hat{\beta}=\left(M^{T} M\right)^{-1} M^{T} Y$ is the maximum likelihood estimator (MLE) of $\beta,$ and $\hat{\sigma}^{2}=n^{-1}(Y-M \hat{\beta})^{T}(Y-M \hat{\beta})$ is the MLE of $\sigma^{2}$

##### Bayesian linear regression with known error variance

Suppose $\sigma^{2}$ is known and the prior for $\beta$ is $N\left(0, V \sigma^{2}\right)$, i.e. $g(\beta) \propto \exp \left\{-0.5\left(\sigma^{2}\right)^{-1} \beta^{T} V^{-1} \beta\right\}$
Setting the prior mean to zero corresponds to the prior. knowledge that each component of $\beta$ is equally likely to be positive or negative, and each component is likely to be near 0 (effect sparsity) 

The posterior of $\beta$ is $g(\beta | Y) \propto L(\beta) g(\beta)$
$=\exp \left\{-\frac{1}{2 \sigma^{2}}(Y-M \beta)^{T}(Y-M \beta)-\frac{1}{2 \sigma^{2}} \beta^{T} V^{-1} \beta\right\}$
$\propto \exp \left\{-\frac{1}{2 \sigma^{2}}(\beta-\tilde{\beta})^{T}\left(M^{T} M+V^{-1}\right)(\beta-\tilde{\beta})\right\}$
where $\tilde{\beta}=\left(M^{T} M+V^{-1}\right)^{-1} M^{T} Y$

Thus, the posterior distribution of $\beta$ is normal. The <u>posterior mean</u> of $\beta$ is $\tilde{\beta}=$ $\left(M^{T} M+V^{-1}\right)^{-1} M^{T} Y,$ and its <u>posterior covariance matrix</u> is $\left(M^{T} M+V^{-1}\right)^{-1} \sigma^{2}$

By choosing $V=\lambda^{-1} I_{p}$ in the Bayesian linear model, we obtain the <u>ridge regression estimator</u> $\tilde{\beta}=\left(M^{T} M+\lambda I_{p}\right)^{-1} M^{T} Y,$ which has desirable frequentist properties.

Suppose that we want to <u>predict at the point $x_{n+1}$</u>. The unobserved response to be predicted is $Y\left(x_{n+1}\right)=m\left(x_{n+1}\right) \beta+\varepsilon_{n+1},$ where $\varepsilon_{n+1} \sim N\left(0, \sigma^{2}\right),$ and $\varepsilon_{n+1}$
is independent of $\varepsilon_{1}, \ldots, \varepsilon_{n} .$ 

- Note that $Y\left(x_{n+1}\right)\left|\beta, Y=Y\left(x_{n+1}\right)\right| \beta \sim N\left(m\left(x_{n+1}\right) \beta, \sigma^{2}\right)$

- We have $Y\left(x_{n+1}\right) | \beta \sim N\left(m\left(x_{n+1}\right) \beta, \sigma^{2}\right) \sim m\left(x_{n+1}\right) \beta+N\left(0, \sigma^{2}\right),$ and $\beta | Y \sim N\left(\left(M^{T} M+\right.\right.$
  $\left.\left.V^{-1}\right)^{-1} M^{T} Y,\left(M^{T} M+V^{-1}\right)^{-1} \sigma^{2}\right) .$ Using the result on the sum of two independent normal random variables, we find that $Y\left(x_{n+1}\right) | Y \sim N\left(m\left(x_{n+1}\right)\left(M^{T} M+V^{-1}\right)^{-1} M^{T} Y,\left[m\left(x_{n+1}\right)\left(M^{T} M+V^{-1}\right)^{-1} m\left(x_{n+1}\right)^{T}+1\right] \sigma^{2}\right)$

- The Bayesian point prediction for $Y\left(x_{n+1}\right)$ is $E\left[Y\left(x_{n+1}\right) | Y\right]=m\left(x_{n+1}\right)\left(M^{T} M+V^{-1}\right)^{-1} M^{T} Y$ and a prediction interval for $Y\left(x_{n+1}\right)$ can be easily constructed as well.

Suppose $V=\lambda^{-1} I_{p},$ <u>how can we choose $\lambda$</u> ?

- One method is to take $\lambda \rightarrow 0$ following the Bayes-Laplace prior $g(\beta) \propto 1$ 

  Jeffrey's prior for $g(\beta)$ is also $g(\beta)=\Pi_{i=1}^{p} g\left(\beta_{i}\right) \propto 1$ since for the Hessian of the log-likelihood with respect to $\beta$ is $\mathcal{H} \log [L(\beta)]=-M^{T} M / \sigma^{2},$ which means that the Fisher's information for each $\beta_{i}$ with the other $\beta_{j}, j \neq i$ fixed is a constant. 

  In this case, $\beta | Y \sim N\left(\left(M^{T} M\right)^{-1} M^{T} Y,\left(M^{T} M\right)^{-1} \sigma^{2}\right),$ which agrees with frequentist inference. The posterior mean is the least squares estimator $\hat{\beta}$ and the posterior covariance matrix of $\beta$ is the covariance matrix of $\hat{\beta}$ under repeated sampling.

- We can also use the <u>empirical Bayes method to select $\lambda$</u> : maximize the marginalized likelihood.

  The empirical Bayes method is a method for specifying parameters of the prior distribution by maximizing the marginalized likelihood, which is proportional to the unconditional distribution (PDF or PMF) of the data. As such, this method chooses the prior distribution parameters based on the data, which means that it does not utilize prior information to specify the prior distribution parameters. Hence, strictly speaking, the empirical Bayes method is a frequentist method.

$\beta \sim N\left(0, \lambda^{-1} I_{p} \sigma^{2}\right), Y | \beta \sim N\left(M \beta, I_{n} \sigma^{2}\right)
$
Thus, the <u>unconditional distribution of the data</u> is $Y \sim N\left(0,\left(\lambda^{-1} M M^{T}+I_{n}\right) \sigma^{2}\right)$ and the <u>log​ marginalized likelihood</u> is $l(\lambda)=-\frac{1}{2} \log \left(\left|\lambda^{-1} M M^{T}+I_{n}\right|\right)-\frac{1}{2 \sigma^{2}} Y^{T}\left(\lambda^{-1} M M^{T}+I_{n}\right)^{-1} Y$
We cannot get an analytical formula for the maximizer of $l(\lambda)$ but we can find the maximizer numerically using optimization algorithms in Matláb like fmincon and patternsearch.

##### Bayesian linear regression with unknown error variance

Consider the linear regression problem $Y=M \beta+\varepsilon$, where $M$ is an $n \times p$ matrix with full column rank, $\beta$ is a $p \times 1$ vector of <u>unknown parameters</u>, and $\varepsilon \sim N\left(0, \sigma^{2} I_{n}\right),$ i.e., $\varepsilon$ is a multivariate normal distribution with a $n \times 1$ mean vector of $0^{\prime} s,$ and a covariance matrix equal to $\sigma^{2}$ times the $n \times n$ identity matrix.

Assume the conjugate prior distribution $g\left(\beta, \sigma^{2}\right)=g\left(\beta | \sigma^{2}\right) g\left(\sigma^{2}\right) \propto$ $\left(\sigma^{2}\right)^{-p / 2}|V|^{-1 / 2} \exp \left\{-0.5\left(\sigma^{2}\right)^{-1} \beta^{T} V^{-1} \beta\right\}\left(\sigma^{2}\right)^{-a-1} \exp \left\{-0.5\left(\sigma^{2}\right)^{-1} b^{-1}\right\}$
where $V$ is a $p \times p$ strictly positive definite matrix, and $a, b$ are strictly positive constants.

We have the following results:

The marginal posterior distribution of $\sigma^{2}$ is an inverse gamma distribution IG $\left(\frac{n}{2}+a, 2\left(b^{-1}+\mathrm{RSS}\right)^{-1}\right)$ distribution, where $\mathrm{RSS}=$ $Y^{T}\left(I-M\left(M^{T} M+V^{-1}\right)^{-1} M^{T}\right) Y$

The marginal posterior distribution of $\beta$ is a multivariate noncentral $t$ distribution with mean $\tilde{\beta}=\left(M^{T} M+V^{-1}\right)^{-1} M^{T} Y$, scale matrix $\left(b^{-1}+\operatorname{RSS}\right)\left(M^{T} M+V^{-1}\right)^{-1} /(n+2 a),$ and $n+2 a$ degrees of freedom.

- Note: A **multivariate noncentral $t$ distribution** with a $p \times 1$ mean vector $\mu,$ scale matrix $S$ (which is positive definite), and $\nu$ degrees of freedom has PDF $f(y) \propto\left[1+v^{-1}(y-\mu)^{T} S^{-1}(y-\mu)\right]^{-(v+p) / 2}$
  Suppose that $X \sim N(0, S), W \sim \chi_{v}^{2}$ (a chi-square distribution with $v$ degrees of freedom), and $X$ and $W$ are independent. Then, $Y=\frac{X}{\sqrt{W / v}}+\mu$
  has the noncentral $t$ distribution described above.
  If $p=1,$ then $(y-\mu) / \sqrt{S}$ has the usual student $t$ distribution.

The likelihood is $L\left(\beta, \sigma^{2}\right) \propto \sigma^{-n} \exp \left[-\frac{1}{2 \sigma^{2}}(Y-M \beta)^{T}(Y-M \beta)\right]$ and the posterior is
$g\left(\beta, \sigma^{2} | Y\right) \propto L\left(\beta, \sigma^{2}\right) g\left(\beta, \sigma^{2}\right)$
$\propto\left(\sigma^{2}\right)^{-\frac{n}{2}} \exp \left[-\frac{1}{2 \sigma^{2}}(Y-M \beta)^{T}(Y-M \beta)\right]$
$\left(\sigma^{2}\right)^{-p / 2} \exp \left\{-0.5\left(\sigma^{2}\right)^{-1} \beta^{T} V^{-1} \beta\right\}\left(\sigma^{2}\right)^{-a-1} \exp \left\{-0.5\left(\sigma^{2}\right)^{-1} b^{-1}\right\}$
$\propto\left(\sigma^{2}\right)^{-\frac{p}{2}} \exp \left[-\frac{1}{2 \sigma^{2}}(\beta-\tilde{\beta})^{T}\left(M^{T} M+V^{-1}\right)(\beta-\tilde{\beta})\right]$
$\left(\sigma^{2}\right)^{-\frac{n+2 a}{2}-1} \exp \left\{-0.5\left(\sigma^{2}\right)^{-1}\left(b^{-1}+Y^{T} Y-Y^{T} M\left(M^{T} M+V^{-1}\right)^{-1} M^{T} Y\right)\right\}$
$\propto\left(\sigma^{2}\right)^{-\frac{p}{2}} \exp \left[-\frac{1}{2 \sigma^{2}}(\beta-\tilde{\beta})^{T}\left(M^{T} M+V^{-1}\right)(\beta-\tilde{\beta})\right]\left(\sigma^{2}\right)^{-\frac{n+2 a}{2}-1} \exp \left\{-0.5\left(\sigma^{2}\right)^{-1}\left[b^{-1}+\operatorname{RSS}\right]\right\}$
This last equation implies that $\beta, \sigma^{2} | Y \sim N\left(\tilde{\beta},\left(M^{T} M+V^{-1}\right)^{-1} \sigma^{2}\right) \mathrm{IG}\left(\frac{n}{2}+a, 2\left(b^{-1}+\mathrm{RSS}\right)^{-1}\right)$

It is easy to see that integrating out $\beta$ from $g\left(\beta, \sigma^{2} | Y\right)$ gives $g\left(\sigma^{2} | Y\right),$ which is clearly the PDF of $\mathrm{IG}\left(\frac{n}{2}+a, 2\left(b^{-1}+\mathrm{RSS}\right)^{-1}\right)$

Using the fact that $\int_{0}^{\infty} z^{-a-1} \exp \left(-\frac{1}{z b}\right)=b^{a} \Gamma(a),$ we can integrate $\sigma^{2}$ out of
$g\left(\beta, \sigma^{2} | Y\right)$ to get $g(\beta | Y) \propto\left[(\beta-\tilde{\beta})^{T}\left(M^{T} M+V^{-1}\right)(\beta-\tilde{\beta})+b^{-1}+\operatorname{RSS}\right]^{-(n+2 a+p) / 2}$
$\propto\left[1+(\beta-\tilde{\beta})^{T}\left(M^{T} M+V^{-1}\right)(\beta-\tilde{\beta}) /\left(b^{-1}+\mathrm{RSS}\right)\right]^{-(n+2 a+p) / 2}$

We see from this last equation that <u>the posterior distribution of $\beta$ is a multivariate noncentral $t$ distribution with mean $\tilde{\beta},$ scale matrix $\left(b^{-1}+\right.$ $\operatorname{RSS}\left(M^{T} M+V^{-1}\right)^{-1} /(n+2 a),$ and $n+2 a$ degrees of freedom.</u>

If we set $V^{-1}=0 I_{p}, b=\infty, a=-p / 2,$ we would get the posterior distribution corresponding to the Jeffrey's prior $g\left(\beta, \sigma^{2}\right)=\prod_{i=1}^{p} g\left(\beta_{i}\right) g\left(\sigma^{2}\right) \propto \sigma^{-2} .$ This is equivalent to the Bayes-Laplace priorfor $\left(\beta, \log \left(\sigma^{2}\right)\right)$, i.e., $g\left(\beta, \log \left(\sigma^{2}\right)\right) \propto 1$ (apply the change of variable formula).

<u>Suppose we want to predict at $x_{n+1}$</u>
$Y\left(x_{n+1}\right)\left|\beta, \sigma^{2}, Y=Y\left(x_{n+1}\right)\right| \beta, \sigma^{2} \sim N\left(m\left(x_{n+1}\right) \beta, \sigma^{2}\right) \sim m\left(x_{n+1}\right) \beta+N\left(0, \sigma^{2}\right)$
$\left.\beta | \sigma^{2}, Y \sim N\left(\tilde{\beta},\left(M^{T} M+V^{-1}\right)^{-1} \sigma^{2}\right) \text { (as seen from } g\left(\beta, \sigma^{2} | Y\right)\right)$
$Y\left(x_{n+1}\right) | \sigma^{2}, Y \sim N\left(m\left(x_{n+1}\right) \tilde{\beta},\left[m\left(x_{n+1}\right)\left(M^{T} M+V^{-1}\right)^{-1} m\left(x_{n+1}\right)^{T}+1\right] \sigma^{2}\right)$
Finally, using the fact that $\sigma^{2} | Y \sim \operatorname{IG}\left(\frac{n}{2}+a, 2\left(b^{-1}+\operatorname{RSS}\right)^{-1}\right),$ and
$f\left(Y\left(x_{n+1}\right) | Y\right)=\int_{0}^{\infty} f\left(Y\left(x_{n+1}\right) | \sigma^{2}, Y\right) g\left(\sigma^{2} | Y\right) d \sigma^{2},$ we get
$f\left(Y\left(x_{n+1}\right) | Y\right) \propto\left[\frac{\left(Y\left(x_{n+1}\right)-m\left(x_{n+1}\right) \tilde{\beta}\right)^{2}}{2\left[m\left(x_{n+1}\right)\left(M^{T} M+V^{-1}\right)^{-1} m\left(x_{n+1}\right)^{T}+1\right]}+\frac{b^{-1}+R S S}{2}\right]^{-\frac{n+2 a+1}{2}}$

$f\left(Y\left(x_{n+1}\right) | Y\right) \propto\left\{1+\frac{\left(Y\left(x_{n+1}\right)-m\left(x_{n+1}\right) \tilde{\beta}\right)^{2}}{\left(b^{-1}+\operatorname{RSS}\right)\left[m\left(x_{n+1}\right)\left(M^{T} M+V^{-1}\right)^{-1} m\left(x_{n+1}\right)^{T}+1\right]}\right\}^{-\frac{n+2 a+1}{2}}$
Thus, we see that $Y\left(x_{n+1}\right) | Y$ is a noncentral $t$ distribution with mean $m\left(x_{n+1}\right) \tilde{\beta},$ scale parameter $\left(b^{-1}+\operatorname{RSS}\right)\left[m\left(x_{n+1}\right)\left(M^{T} M+V^{-1}\right)^{-1} m\left(x_{n+1}\right)^{T}+1\right] /(n+2 a)$ and
degrees of freedom $n+2 a$. This means that $\frac{Y\left(x_{n+1}\right)-m\left(x_{n+1}\right) \tilde{\beta}}{\sqrt{\left[m\left(x_{n+1}\right)\left(M^{T} M+V^{-1}\right)^{-1} m\left(x_{n+1}\right)^{T}+1\right]\left(b^{-1}+\mathrm{RSS}\right) /(n+2 a)}}$
has the usual $t$ -distribution with $n+2 a$ degrees of freedom.

##### Bayesian Probit regression

Suppose we have binary data $\left(x_{1}, Y_{1}\right), \ldots,\left(x_{n}, Y_{n}\right),$ where $x_{i}=\left(x_{i 1}, \ldots, x_{i d}\right)$ and each $Y_{i}$ is either 0 or 1.

Typically, it is assumed that the $Y_{i}^{\prime}$ 's are independent and $\mathbb{P}\left(Y_{i}=0 | \beta\right)=\pi_{i}=$ $e_{\beta}\left(x_{i}\right),$ where the form of the function $e$ is known but the parameters $\beta$ are unknown. In general, $\mathbb{P}(Y(x)=0 | \beta)=\pi=e_{\beta}(x)$

A linear regression model $\pi_{i}=m\left(x_{i}\right) \beta$ is clearly a poor choice since $\pi_{i} \in[0,1]$.

 A better choice is $\pi_{i}=F\left[-m\left(x_{i}\right) \beta\right],$ where $F$ is a CDF Two common choices:

1. Probit regression $F(z)=\Phi(z)=$standard normal $C D F$
2. Logistic regression $F(z)=\frac{1}{1+\exp (-z)}=$standard logistic $C D F$

Likelihood is
$
L(\beta)=\prod_{i=1}^{n}\left[1-F\left(-m\left(x_{i}\right) \beta\right)\right]^{Y_{i}} F\left(-m\left(x_{i}\right) \beta\right)^{1-Y_{i}}
$

We can <u>estimate $\beta$ by the maximum likelihood method</u>. Asymptotic inference based on Fisher's information can be made.

A Bayesian approach can also be used. Suppose that the prior $g(\beta)$ for $\beta$ is $N\left(0, \sigma^{2} I_{p}\right)$ Introduce the latent variables $Z_{i},$ where $Y_{i}=I\left(Z_{i}>0\right),$ i.e., $Y_{i}=1$ if and only if $Z_{i}>0$

since $\mathbb{P}\left(Y_{i}=0 | \beta\right)=F\left(-m\left(x_{i}\right) \beta\right)$ and $\mathbb{P}\left(Y_{i}=0 | \beta\right)=\mathbb{P}\left(Z_{i} \leq 0 | \beta\right),$ it follows that
$\mathbb{P}\left(Z_{i} \leq 0 | \beta\right)=F\left(-m\left(x_{i}\right) \beta\right)$

Note that if $\mathbb{P}(Z \leq z)=F(z),$ then $\mathbb{P}(Z-\mu \leq z-\mu)=F(z-\mu) .$ Define $F(z-\mu)=$
$F_{\mu}(z)$

Thus, supposing that $Z_{i} | \beta$ has the same distribution as $\mathcal{Z}+m\left(x_{i}\right) \beta$ gives $\mathbb{P}\left(Z_{i} \leq z | \beta\right)=$ $F\left(z-m\left(x_{i}\right) \beta\right)=F_{m\left(x_{i}\right) \beta}(z)$ and $\mathbb{P}\left(Y_{i}=0 | \beta\right)=\mathbb{P}\left(Z_{i} \leq 0 | \beta\right)=F\left(-m\left(x_{i}\right) \beta\right)$

We have the following conditional distributions for our Bayesian formulation:

- $Y_{1}, \ldots, Y_{n} | Z_{1}, \ldots, Z_{n}, \beta$ is a degenerate random vector (constant).
  $Z_{1}, \ldots, Z_{n} | \beta \sim \prod_{i=1}^{n} f_{m\left(x_{i}\right) \beta},$ where $f_{m\left(x_{i}\right) \beta}(z)=F_{m\left(x_{i}\right) \beta}^{\prime}(z)=F^{\prime}\left(z-m\left(x_{i}\right) \beta\right)$
  $\beta \sim N\left(0, \sigma^{2} I_{p}\right)$

- Let $A=\left\{i: Y_{i}=0\right\}$ and $B=\left\{i: Y_{i}=1\right\} .$ Then
  $g\left(Z_{1}, \ldots, Z_{n}, \beta | Y_{1}, \ldots, Y_{n}\right) \propto g\left(Z_{1}, \ldots, Z_{n}, Y_{1}, \ldots, Y_{n} | \beta\right) g(\beta) \propto$
  $\prod_{i \in A} f_{m\left(x_{i}\right) \beta}\left(z_{i}\right) I\left(z_{i} \leq 0\right) \prod_{i \in B} f_{m\left(x_{i}\right) \beta}\left(z_{i}\right) I\left(z_{i}>0\right) \exp \left[-\frac{1}{2 \sigma^{2}}\|\beta\|_{2}^{2}\right]$

**Probit regression:** $F$ is the CDF of a standard normal distribution. Then,
$g\left(Z_{1}, \ldots, Z_{n}, \beta | Y_{1}, \ldots, Y_{n}\right)$
$\propto \prod_{i=1}^{n} \exp \left\{-\frac{\left[z_{i}-m\left(x_{i}\right) \beta\right]^{2}}{2}\right\} \Pi_{i \in A} I\left(z_{i} \leq 0\right) \Pi_{i \in B} I\left(z_{i}>0\right) \exp \left[-\frac{1}{2 \sigma^{2}}\|\beta\|_{2}^{2}\right]$
$=\exp \left\{-\|Z-M \beta\|_{2}^{2} / 2\right\} \Pi_{i \in A} I\left(z_{i} \leq 0\right) \Pi_{i \in B} I\left(z_{i}>0\right) \exp \left[-\frac{1}{2 \sigma^{2}}\|\beta\|_{2}^{2}\right]$

We see that $g\left(\beta | Y_{1}, \ldots, Y_{n}, Z_{1}, \ldots, Z_{n}\right)$
$\propto \exp \left\{-\left(\beta-\beta_{*}\right)\left(M^{T} M+\sigma^{-2} I_{p}\right)\left(\beta-\beta_{*}\right) / 2\right\}$
where $\beta_{*}=\left(M^{T} M+\sigma^{-2} I_{p}\right)^{-1} M^{T} Z$

**Gibbs sampling algorithm**:

1. For $i \in A,$ sample $Z_{i}^{t}$ from $N\left(m\left(x_{i}\right) \beta^{t-1}, 1\right)$ truncated from above at 0
2. For $i \in B,$ sample $Z_{i}^{t}$ from $N\left(m\left(x_{i}\right) \beta^{t-1}, 1\right)$ truncated from below at 0
3. Sample $\beta^{t} \sim N\left(\left(M^{T} M+\sigma^{-2} I_{p}\right)^{-1} M^{T} Z^{t},\left(M^{T} M+\sigma^{-2} I_{p}\right)^{-1}\right)$

