---
layout:     post                    # 使用的布局（不需要改）
title:      Bayes笔记一(Basis of Prob)               # 标题
subtitle:   Series notes of SDSC6003 course. #副标题
date:       2020-04-24            # 时间
author:     Qingyu                      # 作者
header-img: img/post-bg-unix-linux.jpg  #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Bayes Theory

---

$\Omega=$ sample space

$\omega=$ outcome

The triple $(\Omega, \mathcal{F}, \mathbb{P})$ is called a probability space.

random variable is a (nice) function $X: \Omega \rightarrow \mathbb{R}$ that assigns a real number $X(\omega)$ to each outcome $\omega .$

If $A$ and $B$ are independent events, then $\mathbb{P}(A | B)=\mathbb{P}(A)$

For any pair of events, $\mathbb{P}(A \cap B)=\mathbb{P}(A | B) \mathbb{P}(B)=\mathbb{P}(B | A) \mathbb{P}(A)$

A discrete random variable has probability mass function (PMF) $f_{X}(x)=\mathbb{P}(X=x)$



**Definition 1.15**: The <u>expected value</u>, or <u>mean</u>, or first moment, of a
random variable $X$ is defined to be

$E(X)=\int_{\mathbb{R}} x d F(x)=\left\{\begin{array}{c}\sum_{x} x f(x), \text { if } X \text { is discrete } \\ \int_{\mathbb{R}} x f(x) d x, \text { if } X \text { is continuous }\end{array}\right.$

assuming that the sum (or integral) is well-defined. We use the following notation to denote the expected value of $X:$ $\mu_{X}=E(X) .$  

- Expectation is a measure of location.

**Definition 1.16**: The <u>variance</u> of a random variable $X$ is defined to be
$\operatorname{var}(X)=E\left[\left(X-\mu_{X}\right)^{2}\right]=\int_{\mathbb{R}}\left(x-\mu_{X}\right)^{2} d F(x)$

- Note: We also denote the variance as $\sigma_{X}^{2}$
- Variance is a measure of spread.

**Definition 1.18**: Let $X$ and $Y$ be random variables with means $\mu_{X}$ and $\mu_{Y}$ and standard deviations $\sigma_{X}$ and $\sigma_{Y} .$ Then, the <u>covariance</u> between $X$ and $Y$ is $\operatorname{cov}(X, Y)=E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]$ and the <u>correlation</u> between $X$ and $Y$ is $\rho_{X, Y}=\operatorname{cov}(X, Y) /\left(\sigma_{X} \sigma_{Y}\right) .$

**Definition 1.19**: The <u>conditional expectation</u> of $X$ given $Y=y$ is $E(X | Y=y)=\int_{\mathbb{R}} x d F_{X | Y}(x | y) .$ If $r(x, y)$ is a function of $x$ and $y,$ then $E(r(X, Y) | Y=y)=\int_{\mathbb{R}} r(x, y) d F_{X | Y}(x | y)$

**Definition 1.20**: The <u>conditional variance</u> is defined as $\operatorname{var}(X | Y= y)=\int_{\mathbb{R}}[x-E(X | Y=y)]^{2} d F_{X | Y}(x | y)$



**Theorem 1.1**: For any events $A$ and $B$, $\mathbb{P}(A \cup B)=\mathbb{P}(A)+\mathbb{P}(B)-$$\mathbb{P}(A \cap B) .$

**Theorem 1.2**: Let $A_{1}, \ldots, A_{k}$ be a partition of $\Omega .$ Then, for any event $B$, $\mathbb{P}(B)=\sum_{i=1}^{k} \mathbb{P}\left(B | A_{i}\right) \mathbb{P}\left(A_{i}\right)$.

**Theorem 1.3**: **Bayes theorem**. Let $A_{1}, \ldots, A_{k}$ be a partition of $\Omega$ and $B$ be an event such that $\mathbb{P}(B)>0 .$ Then, $\mathbb{P}\left(A_{i} |B \right)=\mathbb{P}\left(B | A_{i}\right) \mathbb{P}\left(A_{i}\right) / \sum_{j=1}^{k} \mathbb{P}\left(B | A_{j}\right) \mathbb{P}\left(A_{j}\right)$

**Theorem 1.4**: If two random variables $X$ and $Y$ have the same CDF, then they have the same distribution, i.e., $\mathbb{P}_{X}=\mathbb{P}_{Y}$.

**Theorem 1.5**: A function $F: \mathbb{R} \rightarrow[0,1]$ is a CDF if and only if

1. $x_{1}<x_{2} \Rightarrow F\left(x_{1}\right) \leq F\left(x_{2}\right) .$
2. $\lim _{x \rightarrow-\infty} F(x)=0$ and $\lim _{x \rightarrow \infty} F(x)=1 .$
3. $F$ is right continuous, i.e., $F(x)=\lim _{y \rightarrow x \atop y>x} F(y)$.

**Theorem 1.6**: Let $F$ be the CDF for a random variable $X .$ Then, 

1. $ \mathbb{P}(X=x)=F(x)-F\left(x^{-}\right),$ where $F\left(x^{-}\right)=\lim _{y \rightarrow x} F(y) .$
2. $\mathbb{P}(x<X \leq y)=F(y)-F(x)$.
3. $\mathbb{P}(X>x)=1-F(x)$.
4. If $X$ is continuous, then $F(b)-F(a)=\mathbb{P}(a<X<b)=\mathbb{P}(a \leq X< b)=\mathbb{P}(a<X \leq b)=\mathbb{P}(a \leq X \leq b) .$

**Definition 1.7**: Let $X$ be a random variable with CDF $F$. The <u>inverse CDF</u> or
<u>quantile function</u> is defined by $F^{-1}(q)=\inf \{x: F(x) \geq q\}$ for $q \in(0,1)$.

**Theorem 1.7** : **Factorizable PDF implies independence**. Suppose that $X$ and $Y$ have joint $P D F f$ and $f(x, y)=g(x) h(y) \forall(x, y) \in \mathbb{R}^{2}$ for some functions $g$ and $h,$ then $X$ and $Y$ are independent.

**Theorem 1.8**: Let $Y=r(X)$ be a strictly monotonic function of $X .$ Then, we
can obtain the PDF of $Y$ by the formula:
$f_{Y}(y)=f_{X}\left(r^{-1}(y)\right)\left|\frac{d r^{-1}(y)}{d y}\right|$

**Theorem 1.9**: The rule of the lazy statistician. Let $Y=r(X) .$ Then, $E(Y)=$$E(r(X))=\int_{\mathbb{R}} r(x) d F_{X}(x)$

**Theorem 1.10**: Linearity of expectation. If $X_{1}, \ldots, X_{n}$ are random variables and $a_{1}, \ldots, a_{n}$ are constants, then $E\left(\sum_{i=1}^{n} a_{i} X_{i}\right)=\sum_{i=1}^{n} a_{i} E\left(X_{i}\right)$

 **Theorem 1.11**: Expectation of product of independent random variables. If $X_{1}, \ldots, X_{n}$ are independent random variables, then $E\left(\prod_{i=1}^{n} X_{i}\right)=\prod_{i=1}^{n} E\left(X_{i}\right)$

**Theorem 1.12**: Basic results on variance. Assuming the variance is well defined, it has the following properties:

1. var $(X)=E\left(X^{2}\right)-\mu_{X}^{2}$.
2. If $a$ and $b$ are constants, then var $(a X+b)=a^{2} \operatorname{var}(X)$.
3. If $X_{1}$ and $X_{2}$ are independent, then var $\left(X_{1}+X_{2}\right)=\operatorname{var}\left(X_{1}\right)+\operatorname{var}\left(X_{2}\right)$.

**Theorem 1.13**: The covariance satisfies $\operatorname{cov}(X, Y)=E(X Y)-E(X) E(Y)$ and the correlation satisfies $-1 \leq \rho_{X, Y} \leq 1$

**Theorem 1.14**: Covariance of linear combination. If $X_{1}, \ldots, X_{n}, Y_{1}, \ldots, Y_{n}$ are
random variables, and $a_{1}, \ldots, a_{n}, b_{1}, \ldots, b_{n}$ are constants, then
$\operatorname{cov}\left(\sum_{i=1}^{n} a_{i} X_{i}, \sum_{j=1}^{n} b_{j} Y_{j}\right)=\sum_{i=1}^{n} \sum_{j=1}^{n} a_{i} b_{j} \operatorname{cov}\left(X_{i}, Y_{j}\right)$

**Theorem 1.15**: Covariance matrix of linear transform of a random vector. If $A$
is a matrix and $X$ is a random vector of matching size, then $E(A X)=A E(X)$
and $\operatorname{var}(A X)=\operatorname{Avar}(X) A^{T},$ where $\operatorname{var}(X)$ is the covariance matrix.

**Theorem 1.16**: Expectation of conditional expectation. $E[E(X | Y)]=E(X) .$

**Theorem 1.17**: Law of total variance. $\operatorname{var}(Y)=E[\operatorname{var}(Y | X)]+ \operatorname{var}[E(Y | X)] .$





**Bernoulli distribution**

- $X$ is a Bernoulli random variable if $\mathbb{P}(X=0)=1-\theta, \mathbb{P}(X=1)=\theta$
- Applications: any trial with two possible outcomes.

1. Outcome of a toss of a coin (head or tail).
2. Whether a manufactured product is defective or not.
3. Whether a train arrives on time or not.

**Binomial distribution**

- $X$ is a Binomial random variable, written $X \sim$ Binomial $(n, \theta)$ if
  $f_{X}(k)=\mathbb{P}(X=k)=\left(\begin{array}{c}n \\ k\end{array}\right) \theta^{k}(1-\theta)^{n-k}, k=0,1, \ldots, n .$
- If $X_{1}, \ldots, X_{n}$ are iid Bernoulli random variables, then $X=\sum_{i=1}^{n} X_{i}$ is a Binomial
  random variable.

**Geometric distribution**

- The geometric distribution is the distribution of the number of
  Bernoulli trials until the first success.
- $ f_{X}(n)=\mathbb{P}(X=n)=\theta(1-\theta)^{n-1}, n=1,2, \ldots$

**Negative binomial distribution**

- Number of Bernoulli trials until $r$ th success.
- $X$ is a negative binomial random variable if  $\mathbb{P}(X=n)=\left(\begin{array}{l}n-1 \\ r-1\end{array}\right) \theta^{r}(1-\theta)^{n-r}, n=r, r+1, \dots$

**Poisson distribution**

- $ X$ is a Poisson random variable if
  $\mathbb{P}(X=n)=e^{-\theta} \theta^{n} / n !, n=0,1,2, \ldots$
- It is the limit of the Binomial probability distribution when $n \rightarrow \infty,$ and $n p_{n} \rightarrow \theta:$
  $\left(\begin{array}{c}n \\ k\end{array}\right) p_{n}^{k}\left(1-p_{n}\right)^{n-k}=\lim _{n \rightarrow \infty} \frac{n^{k}+O\left(n^{k-1}\right)}{k !}\left(\frac{\theta+\varepsilon_{n}}{n}\right)^{k}\left(1-\frac{\theta+\varepsilon_{n}}{n}\right)^{n-k}$
- Implication: The Poisson distribution is a suitable model for events that
  have many windows of opportunity for occurrence but the probability of
  occurrence in each window of opportunity is small.

- Suppose $1 . X_{1}$ has a Poisson distribution with parameter $\theta_{1}, 2 . X_{2}$ has a Poisson distribution with parameter $\theta_{2}, 3 . X_{1}$ and $X_{2}$ are independent. Then, $X_{1}+X_{2}$ has a Poisson distribution with parameter $\theta_{1}+\theta_{2}$.

**Normal distribution**

- A random variable $X$ with probability density function
  $f(x)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}, x \in \mathbb{R} .$
  is said to have a normal distribution denoted by $N\left(\mu, \sigma^{2}\right) .$
- The parameter $\mu$ is called the <u>mean</u> and the parameter $\sigma^{2}$ is called
  the <u>variance</u>.

- A normal random variable with mean $\mu=0$ and variance $\sigma^{2}=1$ is
  called a <u>standard normal random variable</u> and is denoted as $Z$.
- Symmetric density: $\mathbb{P}(Z \leq z)=1-\mathbb{P}(Z \leq-z)$

- If $X$ is a normal random variable with mean $\mu$ and variance $\sigma^{2}>0,$ then $Z=\frac{X-\mu}{\sigma}$
  is a standard normal random variable. Thus, any probability problem involving a normal random variable can be transformed into a problem involving a standard normal random variable.

**Exponential distribution**

- A random variable $X$ is said to have an exponential distribution with rate parameter $\theta$, written $X \sim \operatorname{Exp}(\theta)$ if it has density function $f(x)=\theta e^{-\theta x} I(x \geq 0)$.

**Uniform distribution**

- A random variable $X$ is said to be uniformly distributed over the  interval $[a, b]$ if it has density function $f(x)=\left\{\begin{array}{l}\frac{1}{b-a}, a \leq x \leq b \\ 0, \text { otherwise }\end{array}\right.$
- Notation $X \sim \operatorname{unif}([a, b])$.
- Perhaps the most important application of the uniform distribution is the generation of random numbers for computer simulations.

**Gamma distribution**

- A random variable $X$ is said to have a gamma distribution with shape parameter $k$
  and scale parameter $\theta,$ i.e., $X \sim \operatorname{Gamma}(k, \theta)$ if it has probability density function
  $f(x)=\frac{1}{\Gamma(k) \theta^{k}} x^{k-1} e^{-x / \theta}, x \geq 0$

**Multinomial distribution**

$f(x)=\left(\begin{array}{c}n \\ x_{1}, \ldots, x_{k}\end{array}\right) \theta_{1}^{x_{1}} \cdots \theta_{k}^{x_{k}},$ where $\left(\begin{array}{c}n \\ x_{1}, \ldots, x_{k}\end{array}\right)=\frac{n !}{x_{1} ! \cdots x_{k} !}$
Suppose that $X \sim$ Multinomial $(n, \theta) .$ Then, the marginal distribution of $X_{i}$ is Binomial $\left(n, \theta_{i}\right) .$

**Multivariate normal distribution**

- We say that $X=\left(X_{1}, \ldots, X_{k}\right)^{T}$ has a nondegenerate multivariate normal distribution with mean vector $\mu=\left(\mu_{1}, \ldots, \mu_{k}\right)$ and covariance matrix $\Sigma=\left(\begin{array}{ll}\Sigma_{11} & \ldots & \Sigma_{1 k} \\ \vdots & \vdots & \vdots \\ \Sigma_{k 1} & \cdots & \Sigma_{k k}\end{array}\right)$ if it has density
  $f(x ; \mu, \Sigma)=(2 \pi)^{-k / 2}|\Sigma|^{-1 / 2} \exp \left[-0.5(X-\mu)^{T} \Sigma^{-1}(X-\mu)\right]$

  where $\Sigma$ is a symmetric positive definite matrix.

- Note: A symmetric matrix $\Sigma$ is a positive definite matrix if and only if $x^{T} \Sigma x>0$
  for all real vectors $x \neq 0$.

- More generally, $X$ has a multivariate normal distribution with mean vector $\mu$ and
  covariance matrix $\Sigma=A A^{T}$, written $X \sim N(\mu, \Sigma)$ if $X$ has the same distribution as
  $A Z+\mu,$ where $Z$ is a vector of independent standard normal random variables.

**Expectation and variance of some common random variables**

- If $X$ is a Bernoulli random variable, $E(X)=p,$ and its variance is $\operatorname{var}(X)=p(1-p) .$
- If $X \sim$ Binomial $(n, p), E(X)=n p$ and $\operatorname{var}(X)=n p(1-p)$
- If $X$ has a geometric distribution, $E(X)=\frac{1}{\theta}, \operatorname{var}(X)=\frac{1-\theta}{\theta^{2}}$
- If $X$ is a Poisson random variable, its mean is $E(X)=\theta$ and its variance is $\operatorname{var}(X)=\theta$
- If $X$ has a normal distribution, its mean is $E(X)=\mu$ and its variance is $\operatorname{var}(X)=$ $\sigma^{2}$
- If $X$ has an exponential distribution, $E(X)=\theta^{-1}, \operatorname{var}(X)=\theta^{-2}$.
- If $X$ has a Gamma distribution, $E(X)=k \theta, \operatorname{var}(X)=k \theta^{2}$.
- If $X=\left(X_{1}, \ldots, X_{k}\right) \sim N(\mu, \Sigma),$ then $E(X)=\mu$ and $\operatorname{cov}\left(X_{i}, X_{j}\right)=\Sigma_{i j}$.