---
layout:     post                    # 使用的布局（不需要改）
title:      Bayes笔记六(Multivariate normal A、Laplace A)               # 标题
subtitle:   Series notes of SDSC6003 course. #副标题
date:       2020-04-28              # 时间
author:     Qingyu                      # 作者
header-img: img/post-bg-unix-linux.jpg  #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Bayes Theory
---

**Multivariate normal approximation to posterior**

Consider the posterior PDF $g(\theta | x) \propto L(\theta) g(\theta),$ where the proportionality constant is not known.
Write $L(\theta) g(\theta)=\exp [-\psi(\theta)]$ and let $\hat{\theta}$ be the maximum point of $-\psi(\theta)$ that satisfies $\nabla \psi(\hat{\theta})=0$

Denote the Hessian of $\psi(\theta)$ as $\mathcal{H} \psi(\theta) .$ A second order Taylorapproximation to $-\psi(\theta)$ gives:

$L(\theta) g(\theta) \approx \exp \left[-\psi(\hat{\theta})-(\theta-\hat{\theta})^{T} \mathcal{H} \psi(\hat{\theta})(\theta-\hat{\theta}) / 2\right]$

<u>We see that this gives a multivariate normal approximation to the posterior</u> $g(\theta | x): \theta | x \sim N\left(\hat{\theta},[\mathcal{H} \psi(\hat{\theta})]^{-1}\right)$

- The multivariate normal approximation is good when is large due to asymptotic normality of the $MLE$ and the fact that when $n$ is large, the posterior will be dominated by the likelihood.

<u>Suppose that</u> $X_{1}, \ldots, X_{n} \sim \operatorname{Gamma}(\gamma, \phi),$ i.e., the likelihood is $L(\gamma, \phi)=\frac{1}{[\Gamma(\gamma)]^{n} \phi^{n \gamma}}(\ddot{x})^{\gamma-1} \exp \left(-\frac{\dot{x}}{\phi}\right)$
where $\dot{x}=\sum_{i=1}^{n} x_{i}$ and $\ddot{x}=\prod_{i=1}^{n} x_{i} .$

Let the prior for $(\gamma, \phi)$ be given by $g(\gamma, \phi) \propto \exp (-a \gamma-b \phi),$ i.e., $\gamma$ and $\phi$ have independent exponential prior distributions with rate parameters $a$ and $b$ respectively.

$g(\gamma, \phi | x) \propto L(\gamma, \phi) g(\gamma, \phi)=\frac{1}{[\Gamma(\gamma)]^{n} \phi^{n \gamma}}(\ddot{x})^{\gamma-1} \exp \left(-\frac{\dot{x}}{\phi}\right) \exp (-a \gamma-b \phi)$

Thus, $\psi(\gamma, \phi)=n \log (\Gamma(\gamma))+n \gamma \log (\phi)-(\gamma-1) \log (\ddot{x})+\dot{x} / \phi+a \gamma+b \phi$

The gradient is
$\nabla \psi(\gamma, \phi)=\left(\begin{array}{c}\frac{\partial \psi}{\partial \gamma} \\ \frac{\partial \psi}{\partial \phi}\end{array}\right)=\left(\begin{array}{c}n \varphi(\gamma)+n \log (\phi)-\log (\ddot{x})+a \\ n \gamma / \phi-\dot{x} / \phi^{2}+b\end{array}\right)$
where $\varphi(\gamma)=\frac{d}{d x} \log (\Gamma(\gamma))$ is the **digamma function**. The digamma function satisfies the recurrence relation $\psi(x+1)=\psi(x)+\frac{1}{x}$

Note that $(\gamma, \phi) \in[0, \infty)^{2}$. since $\lim _{\gamma \rightarrow 0^{+}} \varphi(\gamma)=-\infty$ and $\lim _{\gamma \rightarrow \infty} \varphi(\gamma)=\infty$, we see that $\frac{\partial \psi}{\partial \gamma}=0$ has a unique solution $\hat{\gamma}>0, \frac{\partial \psi}{\partial \gamma}<0$ when $\gamma<\hat{\gamma},$ and $\frac{\partial \psi}{\partial \gamma}>0$ when $\gamma>\hat{\gamma}$ for any fixed $\phi>0 .$

Similarly, for any fixed $\gamma>0, \frac{\partial \psi}{\partial \phi}=0$ has a unique solution $\hat{\phi}>0, \frac{\partial \psi}{\partial \phi}<0$ when $\phi<\hat{\phi},$ and $\frac{\partial \psi}{\partial \phi}>0$ when $\phi>\hat{\phi}$.

As this is the unique stationary point and $\psi(\gamma, \phi)$ tends to infinity at the boundary, $\psi$ has a global minimum at $(\gamma, \phi)=(\hat{\gamma}, \hat{\phi})$ and $\nabla \psi(\hat{\gamma}, \hat{\phi})=0$

We have
$\mathcal{H} \psi(\theta)=\left(\begin{array}{cc}n \varphi^{\prime}(\gamma) & n / \phi \\ n / \phi & -n \gamma / \phi^{2}+2 \dot{x} / \phi^{3}\end{array}\right)$
where $\varphi^{\prime}(\gamma)$ is the polygamma function of order one. Note that the Hessian only depends on the data through $\dot{x}$ (its sum).

Digamma function at $z$ can be evaluated with $\mathrm{psi}( z)$ in Matlab.
Polygamma function of order $k$ at $z$ can be evaluated with $\mathrm{psi}(\mathrm{k}, z)$.

For complicated problems, it is possible to numerically find the minimum $\hat{\theta}$ of $\psi(\theta)$ and the Hessian at the minimum point $\mathcal{H} \psi(\hat{\theta})$ using the fminunc function in matlab.



**Laplace approximation**

<u>This method yields an approximation to the posterior expectation</u>:
$E[e(\theta) | x]=\int_{\mathbb{R}^{d}} e(\theta) g(\theta | x) d \theta,$ where $e>0$

It is based on the approximation: 
$I=\int_{\mathbb{R}} \exp [-n \psi(\theta)] d \theta \approx \int_{\mathbb{R}} \exp \left[-n \psi(\hat{\theta})-n \psi^{\prime \prime}(\hat{\theta})(\theta-\hat{\theta})^{2} /\right. 2] d \theta=\exp [-n \psi(\hat{\theta})](2 \pi)^{1 / 2}\left[n \psi^{\prime \prime}(\hat{\theta})\right]^{-1 / 2},$ where $\hat{\theta}$ is the maximum point of $-\psi(\theta)$ that satisfies $\psi^{\prime}(\hat{\theta})=0$

The proportionality constant for $g(\theta | x) \propto L(\theta) g(\theta)$ is not known. Thus, we need to compute
$E[e(\theta) | x]=\int_{\mathbb{R}^{d}} e(\theta) L(\theta) g(\theta) d \theta / \int_{\mathbb{R}^{d}} L(\theta) g(\theta) d \theta$

$\operatorname{Set}-n \psi(\theta)=\ln e(\theta)+\ln L(\theta)+\ln g(\theta)$ to get the approximation $\exp [-n \psi(\hat{\theta})](2 \pi)^{1 / 2}\left[n \psi^{\prime \prime}(\hat{\theta})\right]^{-1 / 2}$ for the numerator.
$\operatorname{Set}-n \tilde{\psi}(\theta)=\ln L(\theta)+\ln g(\theta)$ to get the approximation $\exp \left[-n \tilde{\psi}\left(\theta^{*}\right)\right](2 \pi)^{1 / 2}\left[n \tilde{\psi}^{\prime \prime}\left(\theta^{*}\right)\right]^{-1 / 2}$ for the denominator, where $\theta^{*}$ maximizes $-\tilde{\psi}(\theta)$

The Laplace approximation is: $E[e(\theta) | x]=\frac{\exp [-n \psi(\widehat{\theta})]\left[\psi^{\prime \prime}(\widehat{\theta})\right]^{-1 / 2}}{\exp \left[-n \widetilde{\psi}\left(\theta^{*}\right)\right]\left[\widetilde{\psi}^{\prime \prime}\left(\theta^{*}\right)\right]^{-1 / 2}}+O\left(n^{-2}\right)$

<u>Suppose that</u> $X_{1}, \ldots, X_{n}$ are iid Poisson random variables with mean $\theta,$ and suppose that $\theta$ has prior $g(\theta) \propto \theta \exp \left(-\frac{\theta^{2}}{2 \eta^{2}}\right) I(\theta>0)$ (a Rayleigh distribution). Use the Laplace approximation to find $E(\theta | x)$.
We have
$g(\theta | x) \propto L(\theta) g(\theta) \propto \theta^{n \bar{x}} \exp (-n \theta) \theta \exp \left(-\frac{\theta^{2}}{2 \eta^{2}}\right) I(\theta>0)$
$\propto \theta^{n \bar{x}+1} \exp \left(-n \theta-\frac{\theta^{2}}{2 \eta^{2}}\right) I(\theta>0)$
$\propto \theta^{n \bar{x}+1} \exp \left(-\frac{\left(\theta+n \eta^{2}\right)^{2}}{2 \eta^{2}}\right) I(\theta>0)$

Consider
$-n \psi(\theta)=\ln \theta+\ln L(\theta)+\ln g(\theta)=(n \bar{x}+2) \ln \theta-\frac{\left(\theta+n \eta^{2}\right)^{2}}{2 \eta^{2}}+\ln I(\theta>0)$

We have
$-n \psi^{\prime}(\theta)=\frac{n \bar{x}+2}{\theta}-\frac{\theta+n \eta^{2}}{\eta^{2}},-n \psi^{\prime \prime}(\theta)=-\frac{n \bar{x}+2}{\theta^{2}}-\frac{1}{\eta^{2}}$

since $-n \psi^{\prime \prime}(\theta)<0 \forall \theta>0,-n \psi^{\prime \prime}(\theta)$ has a unique global maximum at $\theta=\hat{\theta}$.
$-n \psi^{\prime}(\theta)=\frac{n \bar{x}+2}{\widehat{\theta}}-\frac{\widehat{\theta}+n \eta^{2}}{\eta^{2}}=0 \Leftrightarrow \hat{\theta}^{2}+n \eta^{2} \widehat{\theta}-\eta^{2}(n \bar{x}+2)=0$

Thus, $\hat{\theta}=-\frac{1}{2} n \eta^{2}+\frac{1}{2} \sqrt{n^{2} \eta^{4}+4 \eta^{2}(n \bar{x}+2)}$

Similarly,
$-n \tilde{\psi}(\theta)=\ln L(\theta)+\ln g(\theta)=(n \bar{x}+1) \ln \theta-\frac{\left(\theta+n \eta^{2}\right)^{2}}{2 \eta^{2}}+\ln I(\theta>0)$

This yields $-n \tilde{\psi}^{\prime \prime}(\theta)=-\frac{n \bar{x}+1}{\theta^{2}}-\frac{1}{\eta^{2}}$ and
$\theta^{*}=-\frac{1}{2} n \eta^{2}+\frac{1}{2} \sqrt{n^{2} \eta^{4}+4 \eta^{2}(n \bar{x}+1)}$

It follows that
$E(\theta | x) \approx \frac{\exp [-n \psi(\widehat{\theta})]\left[\psi^{\prime \prime}(\hat{\theta})\right]^{-\frac{1}{2}}}{\exp \left[-n \widetilde{\psi}\left(\theta^{*}\right)\right]\left[\widetilde{\psi}^{\prime \prime}\left(\theta^{*}\right)\right]^{-\frac{1}{2}}}=\hat{\theta}^{n \bar{x}+2} \theta^{*-(n \bar{x}+1)} \exp \left(\frac{-\left(\widehat{\theta}+n \eta^{2}\right)^{2}+\left(\theta^{*}+n \eta^{2}\right)^{2}}{2 \eta^{2}}\right) \sqrt{\left(\frac{n \bar{x}+1}{\theta^{* 2}}+\frac{1}{\eta^{2}}\right) /\left(\frac{n \bar{x}+2}{\widehat{\theta}^{2}}+\frac{1}{\eta^{2}}\right)}$

