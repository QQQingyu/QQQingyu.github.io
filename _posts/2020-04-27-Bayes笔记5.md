---
layout:     post                    # 使用的布局（不需要改）
title:      Bayes笔记五(Bayesian Inference in problems——Models)               # 标题
subtitle:   Series notes of SDSC6003 course. #副标题
date:       2020-04-27              # 时间
author:     Qingyu                      # 作者
header-img: img/post-bg-unix-linux.jpg  #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Bayes Theory
---

**Binomial-Beta Model**

**Motivating problem**: Suppose a biologist intends to estimate the fraction of rats in a geographical area that carries a certain strain of virus. To obtain the data she needs, she captures $m_{1}$ rats, checks the rats for the virus strain of interest, records the number $X_{1}$ of rats that carries the virus strain, and release them back from where they were captured. This process is repeated $n$ times, where $m_{i}$ rats are captured the $i$ th time. 

<u>Sampling model</u>: $X_{i} | \theta \sim$ Binomial $\left(m_{i}, \theta\right), i=1, \ldots, n,$ where each $m_{i}$ is known and $\theta$ is the probability of success (rat carries the virus).

<u>Prior</u>: $\theta \sim \operatorname{Beta}(a, b),$ i.e., $g(\theta)=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \theta^{a-1}(1-\theta)^{b-1} I(0<\theta<1)$

<u>Prior mean</u> $=\frac{a}{a+b},$ <u>prior mode</u> $=\frac{a-1}{a+b-2}$ if $a, b>1$

<u>Likelihood</u>: $L(\theta)=\prod_{i=1}^{n} \theta^{x_{i}}(1-\theta)^{m_{i}-x_{i}}=\theta^{\dot{x}}(1-\theta)^{\dot{m}-\dot{x}}$ where $\dot{x}=\sum_{i=1}^{n} x_{i}$ and $\dot{m}=\sum_{i=1}^{n} m_{i}$

<u>Posterior</u>: $g(\theta | x) \propto L(\theta) g(\theta)=\theta^{\dot{x}+a-1}(1-\theta)^{\dot{m}-\dot{x}+b-1}$

Thus, we see that $\theta | x \sim \operatorname{Beta}(\dot{x}+a, \dot{m}-\dot{x}+b)$

<u>Posterior mean</u>: $E(\theta | x)=\frac{\dot{x}+a}{\dot{m}+a+b}$

<u>Posterior mode</u>: $\frac{\dot{x}+a-1}{\dot{m}+a+b-2}$ if $\dot{x}+a, \dot{m}-\dot{x}+b>1$

<u>Shortest credible interval</u>
$C_{L}=\{\theta: g(\theta | x) \geq L\}=\left[\theta_{l}, \theta_{u}\right]$
$P\left(\theta \in C_{L^{*}}\right)=0.95$
By using the Matlab script given in the right, we find that $L^{*}=0.92444$, and $C_{L^{*}}=[0.064358,0.32194] .$
The length of shortest credible interval is $0.25758 .$ Using the betainv Matlab built-in function, it can also be found that the $95 \%$ equal-tail-probability credible interval for $\theta$ is $[0.07452,0.33727]$. The length of this credible interval is 0.26275 .

<u>Suppose we want to predic</u>t $Y_{1}, \ldots, Y_{k},$ where $Y_{i} | \theta \sim$ Binomial $\left(n_{i}, \theta\right), i=1, \ldots, k,$ and $Y_{1}, \ldots, Y_{k}$ are assumed to be independent.
$f\left(y_{1}, \ldots, y_{k} | \theta\right)=\prod_{i=1}^{k}\left(\begin{array}{c}n_{i} \\ y_{i}\end{array}\right) \theta^{y_{i}}(1-\theta)^{n_{i}-y_{i}}=\left[\prod_{i=1}^{k}\left(\begin{array}{c}n_{i} \\ y_{i}\end{array}\right)\right] \theta^{\dot{y}}(1-\theta)^{\dot{n}-\dot{y}}$
Recall that the posterior for $\theta$ is $g(\theta | x)=\frac{\Gamma(\dot{m}+a+b)}{\Gamma(\dot{x}+a) \Gamma(\dot{m}-\dot{x}+b)} \theta^{\dot{x}+a-1}(1-\theta)^{\dot{m}-\dot{x}+b-1}$
since $\int_{0}^{1} \theta^{a-1}(1-\theta)^{b-1} d \theta=\frac{\Gamma(a) \Gamma(b)}{\Gamma(a+b)},$ we have
$f\left(y_{1}, \ldots, y_{k} | x\right)=\int_{0}^{1} f\left(y_{1}, \ldots, y_{k} | \theta\right) g(\theta | x) d \theta$
$=\left[\prod_{i=1}^{k}\left(\begin{array}{c}n_{i} \\ y_{i}\end{array}\right)\right] \frac{\Gamma(\dot{y}+\dot{x}+a) \Gamma(\dot{n}-\dot{y}+\dot{m}-\dot{x}+b)}{\Gamma(\dot{n}+\dot{m}+a+b)} \frac{\Gamma(\dot{m}+a+b)}{\Gamma(\dot{x}+a) \Gamma(\dot{m}-\dot{x}+b)}$
$=\left[\prod_{i=1}^{k}\left(\begin{array}{c}n_{i} \\ y_{i}\end{array}\right)\right] \frac{\mathrm{B}(\dot{y}+\dot{x}+a, \dot{n}-\dot{y}+\dot{m}-\dot{x}+b)}{\mathrm{B}(\dot{x}+a, \dot{m}-\dot{x}+b)}$



**Poisson-Gamma Model**

**Motivating problem**: Suppose a quality engineer is interested to study the distribution of the number of dead pixels on each batth of 1000 laptop. To obtain the data she needs, she inspects $n$ batches that each consists of 1000 laptops, and records the numbers $X_{1}, \ldots, X_{n}$ of defective pixels found in each batch. The Poisson distribution is an appropriate model for this problem, and generally for modeling the number of occurrences of a phenomenon where there are many independent windows of opportuities for ocurrence but the probability of the phenomenon occurring in a window of opportunity is a small constant.

<u>Sampling mode</u>l: $X_{i} | \theta \sim$ Poisson $(\theta), i=1, \ldots, n,$ where $\theta$ is the mean.

<u>Prior</u>: $\theta \sim \operatorname{Gamma}(a, b),$ i.e., $g(\theta)=\frac{1}{\Gamma(a) b^{a}} \theta^{a-1} e^{-\frac{\theta}{b} }I(\theta>0)$

<u>Prior mean</u> $=a b,$ <u>prior mode</u> $=(a-1) b$ if $a \geq 1$

<u>Likelihood</u>: $L(\theta)=\prod_{i=1}^{n} e^{-\theta} \theta^{x_{i}}=e^{-n\theta} \theta^{\dot{x}}$ where $\dot{x}=\sum_{i=1}^{n} x_{i}$
<u>Posterior</u>: $g(\theta | x) \propto L(\theta) g(\theta)=\theta^{\dot{x}+a-1} e^{-\left(n+\frac{1}{b}\right) \theta}$

Thus, we see that $\theta | x \sim \operatorname{Gamma}\left(\dot{x}+a, \frac{b}{n b+1}\right)$

<u>Posterior mean</u>: $E(\theta | x)=\frac{(\tilde{x}+a) b}{n b+1}$

<u>Posterior mode</u>: $\frac{(x+a-1) b}{n b+1}$ if $\dot{x}+a \geq 1$

<u>Shortest credible interval</u>
$C_{L}=\{\theta: g(\theta | x) \geq L\}=\left[\theta_{l}, \theta_{u}\right]$
$P\left(\theta \in C_{L^{*}}\right)=0.95 .$
By using the Matlab script given in the right, we find that $L^{*}=0.14088$, and $C_{L^{*}}[1.1308,2.7398] .$ The length of shortest credible interval is 1.6089. Using the betainv Matlab build-in function, it can also be found that the 95\% equal-tail-probability credible interval for $\theta$ is $[1.1818,2.808] .$ The length of this credible interval is 1.6263 .

<u>Suppose we want to predict</u> $Y_{1}, \ldots, Y_{k},$ where $Y_{i} | \theta \sim$ Poisson $(\theta), i=1, \ldots, k,$ and $Y_{1}, \ldots, Y_{k}$ are assumed to be independent.
$f\left(y_{1}, \ldots, y_{k} | \theta\right)=\prod_{i=1}^{k} e^{-\theta} \theta^{y_{i}} / y_{i} !=\left(\prod_{i=1}^{k} y_{i} !\right)^{-1} e^{-k \theta} \theta^{j}$
$g(\theta | x)=\frac{(n b+1)^{x+a}}{\Gamma(x+a) b^{\dot{x}+a}} \theta^{\dot{x}+a-1} e^{-\left(\frac{n b+1}{b}\right) \theta}$
since $\int_{0}^{\infty} \theta^{a-1} e^{-\frac{\theta}{b}} d \theta=\Gamma(a) b^{a},$ we have
$f\left(y_{1}, \ldots, y_{k} | x\right)=\int_{0}^{\infty} f\left(y_{1}, \ldots, y_{k} | \theta\right) g(\theta | x) d \theta$
$=\left(\prod_{i=1}^{k} y_{i} !\right)^{-1} \frac{(n b+1)^{x+a}}{\Gamma(x+a) b^{x+a}} \int_{0}^{\infty} \theta^{j+x+a-1} e^{-\left(\frac{k b+n b+1}{b}\right) \theta} d \theta$
$=\left(\prod_{i=1}^{k} y_{i} !\right)^{-1} \frac{\Gamma(j+x+a)}{\Gamma(x+a)}\left(\frac{n b+1}{b}\right)^{x+a}\left(\frac{b}{k b+n b+1}\right)^{y+x+a}$



**Normal (known variance)-Normal Model**

**Motivating problem**: Suppose an engineer is interested to estimate the thickess (in units of micrometer) of a sheet of aluminum foil. To obtain the data she needs, he takes $n$ measurements $X_{1}, \ldots, X_{n}$ using a device with known measurement error variance $\sigma^{2}=2 .$

<u>Sampling model</u>: $X_{i} | \theta \sim \operatorname{Normal}\left(\theta, \sigma^{2}\right), i=1, \ldots, n,$ where $\theta$ is the mean and $\sigma^{2}$ is the known variance.

<u>Prior</u>: $\theta \sim$ Normal $\left(\mu, \rho^{2}\right),$ i.e., $g(\theta)=\frac{1}{\sqrt{2 \pi} \rho} \exp \left[-\frac{1}{2 \rho^{2}}(\theta-\mu)^{2}\right]$

<u>Prior mean</u> $=\mu,$ <u>prior mode</u> $=\mu$

The Bayes-Laplace prior and Jeffrey's prior are both obtained by taking $\rho \rightarrow \infty,$ i.e., $g(\theta) \propto 1 .$

Note: $\prod_{i=1}^{n} \exp \left[-\frac{1}{2 \sigma^{2}}\left(x_{i}-\theta\right)^{2}\right]=\exp \left[-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right] \exp \left[-\frac{n}{2 \sigma^{2}}(\theta-\bar{x})^{2}\right]$ where $\bar{x}=n^{-1} \sum_{i=1}^{n} x_{i}$

<u>Likelihood</u>: $L(\theta)=\exp \left[-\frac{n}{2 \sigma^{2}}(\theta-\bar{x})^{2}\right]$

<u>Posterior</u>: $g(\theta | x) \propto L(\theta) g(\theta)=\exp \left[-\frac{n}{2 \sigma^{2}}(\theta-\bar{x})^{2}-\frac{1}{2 \rho^{2}}(\theta-\mu)^{2}\right] \propto \exp \left[-\frac{1}{2} \omega^{-2}(\theta-\psi)^{2}\right],$ where $\psi=\frac{\frac{n}{\sigma^{2}} \bar{x}+\frac{1}{\rho^{2}} \mu}{\frac{n}{\sigma^{2}}+\frac{1}{\rho^{2}}}$ and $\omega^{2}=\left(\frac{n}{\sigma^{2}}+\frac{1}{\rho^{2}}\right)^{-1}$
Thus, we see that $\theta | x \sim N\left(\psi, \omega^{2}\right) .$ 

The <u>posterior mean</u> is $E(\theta | x)=\psi,$ which is a convex combination of $\bar{x}$ and $\mu$ with weights proportional to the precision (inverse variance) of $\bar{x}$, i.e., $\frac{n}{\sigma^{2}}$ and the precision of the prior guess $\mu,$ i.e., $\frac{1}{\rho^{2}}$. The <u>posterior precision</u> $\omega^{-2}$ is the sum of the precisions of the data-based statistic and the prior. The shortest credible interval is just the equal tail probability credible interval.



**Normal (known mean)-Inverse Gamma Model**

**Motivating problem**: Suppose a measurement system expert intends to assess the precision of a temperature measuring device by repeatedly measuring the temperature of boiling water at atmospheric pressure. The measuring device is well calibrated, which means that the mean measurement is known to be equal to the true value of $\mu=100$ Celsius.

<u>Sampling model</u>: $X_{i} | \theta \sim$ Normal $(\mu, \theta), i=1, \ldots, n,$ where $\theta$ is the variance and $\mu$ is the known mean.

<u>Prior</u>: $\theta \sim \operatorname{InvGamma}(a, b),$ i.e., $g(\theta)=\frac{1}{\Gamma(a) b^{a}} \theta^{-a-1} \exp \left(-\frac{1}{b \theta}\right)$
Note: $1 / \theta \sim \operatorname{Gamma}(a, b)$

<u>Prior mean</u> $=\frac{1}{b(a-1)}, a>1,$ <u>prior mode</u> $=\frac{1}{b(a+1)}$

Bayes-Laplace prior and Jeffrey's prior are obtained with $a=0, b \rightarrow \infty,$ i.e., $g(\theta) \propto \theta^{-1}$.

<u>Likelihood</u>: $L(\theta)=\theta^{-n / 2} \exp \left[-\frac{1}{2 \theta} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\right]$

<u>Posterior</u>: $g(\theta | x) \propto L(\theta) g(\theta)=\theta^{-\frac{n}{2}-a-1} \exp \left\{-\theta^{-1}\left[\frac{1}{2} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}+\right.\right. \left.\left.b^{-1}\right]\right\}$

Thus, we see that $\theta | x \sim \operatorname{InvGamma}\left(\frac{n}{2}+a,\left[\frac{1}{2} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}+b^{-1}\right]^{-1}\right)$

<u>Posterior mean</u>: $E(\theta | x)=\frac{\frac{1}{2} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}+b^{-1}}{\frac{n}{2}+a-1}=\frac{\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}+2 / b}{n+2 a-2}$

<u>Posterior mode</u>: $\frac{\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}+2 / b}{n+2 a+2}$



**Normal-Inverse Gamma Model**

Motivating problem: Suppose it is of interest to estimate the mean and variance $\theta=\left(\mu, \sigma^{2}\right)$ of the weight of a certain species of animal that is appropriately modeled by a normal distribution. A total of $n$ weight measurements of the species of animal is obtained.

<u>Sampling model</u>: $X_{i} | \theta \sim \operatorname{Normal}\left(\mu, \sigma^{2}\right), i=1, \ldots, n .$

<u>Prior</u>: $g\left(\mu, \sigma^{2}\right)=g\left(\mu | \sigma^{2}\right) g\left(\sigma^{2}\right) \propto \left(\sigma^{2} / n_{0}\right)^{-1 / 2} \exp \left\{-0.5\left(\sigma^{2} / n_{0}\right)^{-1}\left(\mu-\mu_{0}\right)^{2}\right\}\left(\sigma^{2}\right)^{-a-1} \exp \left\{-\sigma^{-2} b^{-1}\right\}$
Note: $g\left(\mu | \sigma^{2}\right)$ is Normal $\left(\mu_{0}, \frac{\sigma^{2}}{n_{0}}\right)$ a<u>nd $g\left(\sigma^{2}\right)$ is InvGamma</u> $(a, b)$

The density of a noncentral $t$ distribution with mean $m,$ scale parameter $c^{2}$ and degrees of freedom $v$ is given by $f(x) \propto\left[1+(x-m)^{2} /\left(v c^{2}\right)\right]^{-(1+v) / 2}$ and we write $t\left(m, c^{2}, v\right) .$ The variance of the distribution is $\frac{v}{v-2} c^{2}$.
If $X$ has the above density, then $Z=(X-m) / c$ has the usual $t$ density.

$g(\mu)=\int_{0}^{\infty} g\left(\mu, \sigma^{2}\right) d \sigma^{2} \propto\left[n_{0}\left(\mu-\mu_{0}\right)^{2}+2 / b\right]^{-\left(\frac{2 a+1}{2}\right)} \propto\left\{1+n_{0} b\left(\mu-\mu_{0}\right)^{2} / 2\right\}^{-\left(\frac{2 a+1}{2}\right)}$
Thus, we see that the <u>prior for $\mu$ is noncentral $t$ distribution</u> with mean $\mu_{0}$, scale parameter $1 /\left(a b n_{0}\right)$ and degrees of freedom $2 a$.

Let $\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=n s^{2}$ and $\bar{x}=n^{-1} \sum_{i=1}^{n} x_{i}$
Then, <u>the likelihood</u> is $L\left(\mu, \sigma^{2}\right)=\left(\sigma^{2}\right)^{-n / 2} \exp \left\{-0.5 \sigma^{-2}\left[n s^{2}+n(\bar{x}-\mu)^{2}\right]\right\}$

Thus, the <u>posterior distribution of $\left(\mu, \sigma^{2}\right)$ is</u>
$g\left(\mu, \sigma^{2} | x\right) \propto\left(\sigma^{2}\right)^{-1 / 2}\left(\sigma^{2}\right)^{-(n+2 a+2) / 2} \exp \left\{-0.5 \sigma^{-2}\left[\left(n+n_{0}\right)\left(\mu-\frac{n \bar{x}+n_{0} \mu_{0}}{n+n_{0}}\right)^{2}+\right.\right. \left.\left.n s^{2}+2 / b+\frac{n n_{0}}{n+n_{0}}\left(\bar{x}-\mu_{0}\right)^{2}\right]\right\}$
Thus, $\mu, \sigma^{2} | x \sim \operatorname{Normal}\left(\frac{n \bar{x}+n_{0} \mu_{0}}{n+n_{0}}, \frac{\sigma^{2}}{n+n_{0}}\right) \operatorname{InvGamma}\left(\frac{n}{2}+a, 2\left[n s^{2}+2 / b+\right.\right. \left.\left.\frac{n n_{0}}{n+n_{0}}\left(\bar{x}-\mu_{0}\right)^{2}\right]^{-1}\right),$ where $\mu | x, \sigma^{2} \sim \operatorname{Normal}\left(\frac{n \bar{x}+n_{0} \mu_{0}}{n+n_{0}}, \frac{\sigma^{2}}{n+n_{0}}\right)$

since we know the normalizing constants for the normal and inverse gamma distribution, $g(\theta | x)=\left(2 \pi \sigma_{n}^{2}\right)^{-\frac{1}{2}} \exp \left[-0.5 \sigma_{n}^{-2}\left(\mu-\mu_{n}\right)^{2}\right] \frac{1}{\Gamma\left(a_{n}\right) b_{n}^{a_{n}}}\left(\sigma^{2}\right)^{-a_{n}-1} \exp \left(-\frac{1}{b_{n} \sigma^{2}}\right)$
where $\mu_{n}=\frac{n \bar{x}+n_{0} \mu_{0}}{n+n_{0}}, \sigma_{n}^{2}=\sigma^{2} /\left(n+n_{0}\right), a_{n}=n / 2+a, b_{n}=2\left[n s^{2}+2 / b+\frac{n n_{0}}{n+n_{0}}\left(\bar{x}-\mu_{0}\right)^{2}\right]^{-1}$
It is easily seen that:

 $g\left(\sigma^{2} | x\right)=\int_{\mathbb{R}} g\left(\mu, \sigma^{2} | x\right) d \mu \sim \operatorname{InvGamma}\left(a_{n}, b_{n}\right)$ and the <u>posterior mean</u> of $\sigma^{2}$ is
$\frac{1}{n+2 a-2}\left[n s^{2}+2 / b+\frac{n n_{0}}{n+n_{0}}\left(\bar{x}-\mu_{0}\right)^{2}\right]$

The marginal posterior density for $\mu$ is:
$g(\mu | x)=\int_{0}^{\infty} g\left(\mu, \sigma^{2} | x\right) d \sigma^{2} \propto\left[\left(n+n_{0}\right)\left(\mu-\mu_{n}\right)^{2}+n s^{2}+2 / b+\frac{n n_{0}}{n+n_{0}}\left(\bar{x}-\mu_{0}\right)^{2}\right]^{-\left(\frac{n+1}{2}+a\right)} \propto$
$\left\{1+\left(n+n_{0}\right)\left(\mu-\mu_{n}\right)^{2} /\left[n s^{2}+2 / b+\frac{n n_{0}}{n+n_{0}}\left(\bar{x}-\mu_{0}\right)^{2}\right]\right\}^{-\left(\frac{n+1}{2}+a\right)}$

Thus, $g(\mu | x)$ is a noncentral $t$ distribution with mean $\mu_{n}=\frac{n \bar{x}+n_{0} \mu_{0}}{n+n_{0}},$ scale $c_{n}^{2}=\frac{1}{(n+2 a)\left(n+n_{0}\right)}\left[n s^{2}+\right.$
$\left.2 / b+\frac{n n_{0}}{n+n_{0}}\left(\bar{x}-\mu_{0}\right)^{2}\right],$ and $v_{n}=n+2 a$ degrees of freedom, i.e., $t\left(\mu_{n}, c_{n}^{2}, v_{n}\right)$

<u>Suppose we want to predict</u> $Y_{1}, \ldots, Y_{k},$ where $Y_{i} | \theta \sim$ Normal $\left(\mu, \sigma^{2}\right), i=1, \ldots, k,$ and $Y_{1}, \ldots, Y_{k}$ are assumed to be independent.
$f\left(y_{1}, \ldots, y_{k} | \theta\right)=\left(2 \pi \sigma^{2}\right)^{-k / 2} \exp \left\{-0.5 \sigma^{-2}\left[k r^{2}+k(\bar{y}-\mu)^{2}\right]\right\}$ where $\sum_{i=1}^{k}\left(y_{i}-\bar{y}\right)^{2}=k r^{2} .$
$g(\theta | x)=\left(2 \pi \sigma_{n}^{2}\right)^{-\frac{1}{2}} \exp \left[-0.5 \sigma_{n}^{-2}\left(\mu-\mu_{n}\right)^{2}\right] \frac{1}{\Gamma\left(a_{n}\right) b_{n}^{a_{n}}}\left(\sigma^{2}\right)^{-a_{n}-1} \exp \left(-\frac{1}{b_{n} \sigma^{2}}\right)$
where $\mu_{n}=\frac{n \bar{x}+n_{0} \mu_{0}}{n+n_{0}}, \sigma_{n}^{2}=\sigma^{2} /\left(n+n_{0}\right), a_{n}=n / 2+a, b_{n}=2\left[n s^{2}+2 / b+\right. \left.\frac{n n_{0}}{n+n_{0}}\left(\bar{x}-\mu_{0}\right)^{2}\right]^{-1}$

$\int_{\mathbb{R}} f\left(y_{1}, \ldots, y_{k} | \theta\right) g(\theta | x) d \mu=\left[2 \pi \sigma^{2}\left(1 /\left(k+n+n_{0}\right)\right)\right]^{\frac{1}{2}} \exp \left[\frac{k+n+n_{0}}{2 \sigma^{2}}\left(\frac{k \bar{y}+\left(n+n_{0}\right) \mu_{n}}{k+n+n_{0}}\right)^{2}-\frac{k}{2 \sigma^{2}} \bar{y}^{2}-\frac{n+n_{0}}{2 \sigma^{2}} \mu_{n}^{2}\right]$
$\left(2 \pi \sigma_{n}^{2}\right)^{-\frac{1}{2}}\left(2 \pi \sigma^{2}\right)^{-\frac{k}{2}} \exp \left(-0.5 \sigma^{-2} k r^{2}\right) \frac{1}{\Gamma\left(a_{n}\right) b_{n}^{a_{n}}}\left(\sigma^{2}\right)^{-a_{n}-1} \exp \left(-\frac{1}{b_{n} \sigma^{2}}\right)$
$=\left[\left(n+n_{0}\right) /\left(k+n+n_{0}\right)\right]^{\frac{1}{2}}(2 \pi)^{-k / 2} \frac{1}{\Gamma\left(a_{n}\right) b_{n}^{a_{n}}}\left(\sigma^{2}\right)^{-k / 2-a_{n}-1}\exp \left\{-0.5 \sigma^{-2}\left[\frac{1}{1 / k+1 /\left(n+n_{0}\right)}\left(\bar{y}-\mu_{n}\right)^{2}+k r^{2}+\frac{2}{b_{n}}\right]\right\}$
Further integrating out $\sigma^{2}$ gives
$f\left(y_{1}, \ldots, y_{k} | x\right)=\left[\left(n+n_{0}\right) /\left(k+n+n_{0}\right)\right]^{\frac{1}{2}}(2 \pi)^{-k / 2} \frac{1}{\Gamma\left(a_{n}\right) b_{n}^{a_{n}}} \Gamma\left(\frac{k}{2}+a_{n}\right)\left[\frac{1}{\sqrt[1]{k+1 /\left(n+n_{0}\right)}\left(\bar{y}-\mu_{n}\right)^{2}+k r^{2}+\frac{2}{b_{n}}}\right]^{\frac{k}{2}+a_{n}}$
$f\left(y_{1}, \ldots, y_{k} | x\right)$ looks intimidating! It is a multivariate t density.

<u>Alternative approach to compute predictive density</u>: Monte Carlo sampling from predictive distribution
Step 1: Sample $\sigma^{2} \sim \operatorname{InvGamma}\left(a_{n}, b_{n}\right) .$
Step 2: Sample $\mu \sim \operatorname{Normal}\left(\mu_{n}, \sigma_{n}^{2}\right) .$
Step 3: Sample $Y_{1}, \ldots, Y_{k} \sim$ iid $\operatorname{Normal}\left(\mu, \sigma^{2}\right) .$
Repeat the above three steps $N$ times to get $N$ samples of $Y_{1}, \ldots, Y_{k}$.



**Two independent normal models**

<u>Sampling model</u>:
$X_{1 i} | \theta_{1} \sim \operatorname{Normal}\left(\mu_{1}, \sigma_{1}^{2}\right), i=1, \ldots, n_{1}$
$X_{2 i} | \theta_{2} \sim \operatorname{Normal}\left(\mu_{2}, \sigma_{2}^{2}\right), i=1, \ldots, n_{2}$
$X_{11}, \ldots, X_{1 n_{1}}, X_{21}, \ldots, X_{2 n_{2}}$ are independent.

<u>Prior</u>: $g\left(\mu_{1}, \sigma_{1}^{2}, \mu_{2}, \sigma_{2}^{2}\right)=g\left(\mu_{1}, \sigma_{1}^{2}\right) g\left(\mu_{2}, \sigma_{2}^{2}\right)$
$\mu_{1} | \sigma_{1}^{2} \sim$ Normal $\left(\mu_{01}, \frac{\sigma_{1}^{2}}{n_{01}}\right), \sigma_{1}^{2} \sim \operatorname{InvGamma}\left(a_{1}, b_{1}\right)$
$\mu_{2} | \sigma_{2}^{2} \sim \operatorname{Normal}\left(\mu_{02}, \frac{\sigma_{2}^{2}}{n_{02}}\right), \sigma_{2}^{2} \sim \operatorname{InvGamma}\left(a_{2}, b_{2}\right)$

**Comparing means**:

By results already established for the normal-inverse gamma model, $\mu_{1}\left|x \sim t\left(\mu_{n 1}, c_{n 1}^{2}, v_{n 1}\right), \mu_{2}\right| x \sim t\left(\mu_{n 2}, c_{n 2}^{2}, v_{n 2}\right)$
where $\mu_{1}$ and $\mu_{2}$ are independent given $x,$ and $\mu_{n i}=\frac{n_{i} \bar{x}_{i}+n_{0 i} \mu_{0 i}}{n_{i}+n_{0 i}}, c_{n i}^{2}=\frac{1}{\left(n_{i}+2 a_{i}\right)\left(n_{i}+n_{0 i}\right)}\left[n_{i} s_{i}^{2}+2 / b_{i}+\frac{n_{i} n_{0 i}}{n_{i}+n_{0 i}}\left(\bar{x}_{i}-\mu_{0 i}\right)^{2}\right], v_{n i}=n_{i}+2 a_{i}, i=1,2$
Using this result and the fact that $\frac{\mu_{i}-\mu_{n i}}{c_{n i}} \sim t_{v_{n i}},$ where $t_{v_{n i}}$ is the usual $t$ distribution with $v_{n i}$ degrees of freedom, it is easy to construct credible intervals for $\mu_{1}-\mu_{2}$ via Monte Carlo simulation.

**Motivating problem**: To study the effect of sedentary occupation on cholesterol levels (measured in milligrams per deciliter (mg/dL)), the cholesterol level of $n_{1}=20$ office workers and $n_{2}=20$ non-office workers were measured. We are interested in the difference $\mu_{1}-\mu_{2}$, where $\mu_{1}$ is the mean cholesterol level of office workers and $\mu_{2}$ is the mean cholesterol level of non-office workers.

- It is found that $\bar{x}_{1}=238, \bar{x}_{2}=201, s_{1}^{2}=80,$ and $s_{2}^{2}=70$
- Suppose that independent noninformative priors are used, i.e., $n_{01}= n_{01}=0, a_{1}=a_{2}=-1 / 2,$ and $b_{1}=b_{2}=\infty .$
- We find that:
  $\mu_{n 1}=\frac{n_{1} \bar{x}_{1}+n_{01} \mu_{01}}{n_{1}+n_{01}}=\bar{x}_{1}=238, \mu_{n 2}=\bar{x}_{2}=201$
  $c_{n 1}^{2}=\frac{1}{\left(n_{1}+2 a_{1}\right)\left(n_{1}+n_{01}\right)}\left[n_{1} s_{1}^{2}+2 / b_{1}+\frac{n_{1} n_{01}}{n_{1}+n_{01}}\left(\bar{x}_{1}-\mu_{01}\right)^{2}\right]=\frac{s_{1}^{2}}{n_{1}-1}=\frac{80}{20-1}=4.2105$
  $c_{n 2}^{2}=\frac{s_{2}^{2}}{n_{2}-1}=\frac{70}{20-1}=3.6842$
  $v_{n 1}=n_{1}+2 a_{1}=19, v_{n 2}=19$
  By simulating 10000 values of $\mu_{1}$ from its posterior distribution$(\mathrm{mul}=\text { sqrt }(4.2105) * \text { trnd }(19,10000,1)+238)$ and 10000 values of $\mu_{2}$ from its posterior distribution (mu2=sqrt(3.6842) $\left.^{*} \text { trnd }(19,10000,1)+201\right)$ and taking their differences, we find that a $95 \%$ equal tail probability credible interval for $\mu_{1}-\mu_{2}$ is $[31.095,42.947]$.

**Comparing variances**:

By results already established for the normal-inverse gamma model, $\sigma_{1}^{2}\left|x=\operatorname{InvGamma}\left(a_{n 1}, b_{n 1}\right), \sigma_{2}^{2}\right| x=\operatorname{InvGamma}\left(a_{n 2}, b_{n 2}\right)$
where $\sigma_{1}^{2}$ and $\sigma_{2}^{2}$ are independent given $x,$ and
$a_{n i}=n_{i} / 2+a_{i}, b_{n i}=2\left[n_{i} s_{i}^{2}+2 / b_{i}+\frac{n_{i} n_{0 i}}{n_{i}+n_{0 i}}\left(\bar{x}_{i}-\mu_{0 i}\right)^{2}\right]^{-1}, i=1,2$
It is easy to construct credible intervals for $\sigma_{1}^{2} / \sigma_{2}^{2}$ via Monte Carlo simulation.



**Multinomial-Dirichlet Model**

**Motivating problem**:Consider drawing a ball from an urn which has balls with $l$ different colors. Let $\theta=\left(\theta_{1}, \ldots, \theta_{l}\right),$ where $\theta_{i}$ is the probability of drawing a ball of color $i$ and $\sum_{i=1}^{l} \theta_{i}=1 .$ Suppose that $n$ draws are made with replacement. Let $X_{i}$ be the number of times that color $i$ appears, and let $X=\left(X_{1}, \ldots, X_{l}\right) .$. It must hold that $n=\sum_{i=1}^{l} X_{i} .$

We say that $X$ has a multinomial $(n, \theta)$ distribution, written $X \sim$ Multinomial $(n, \theta) .$ The probability mass function of $X$ is
$f(x | \theta)=\left(\begin{array}{c}n \\ x_{1}, \ldots, x_{l}\end{array}\right) \theta_{1}^{x_{1}} \cdots \theta_{l}^{x_{l}},$ where $\left(\begin{array}{c}n \\ x_{1}, \ldots, x_{l}\end{array}\right)=\frac{n !}{x_{1} ! \cdots x_{l} !}$
There are $\left(\begin{array}{c}n+l-1 \\ l-1\end{array}\right)$ possible values for $X .$

Note that if $X^{i} \sim$ Multinomial $\left(n_{i}, \theta\right), i=1, \ldots, N$ and $X^{1}, \ldots, X^{N}$ are independent, then $X=\sum_{i=1}^{N} X^{i} \sim$ Multinomial $(n, \theta),$ where $n=$$\sum_{i=1}^{N} n_{i},$ and $X$ is a sufficient statistic for $\theta$. Thus, we only need to consider the case where we have data in the form of a vector $X$ from a multinomial random sample of size one (iid samples can always be reduced to this case).
$\operatorname{cov}(X)=n\left(\begin{array}{cccc}\theta_{1}\left(1-\theta_{1}\right) & -\theta_{1} \theta_{2} & \ldots & -\theta_{1} \theta_{l} \\ -\theta_{2} \theta_{1} & \theta_{2}\left(1-\theta_{2}\right) & -\theta_{2} \theta_{l} \\ \vdots & \ddots & \vdots \\ -\theta_{l} \theta_{1} & -\theta_{l} \theta_{2} & \cdots & \theta_{l}\left(1-\theta_{l}\right)\end{array}\right)$

Note that $\theta \in \Theta=\left\{\left(\theta_{1}, \ldots, \theta_{l}\right) \in[0,1]^{l}: \sum_{i=1}^{l} \theta_{i}=1\right\},$ where $\Theta$ is the $l$ -dimensional simplex. The conjugate prior is the **Dirichlet distribution**. It has support on this simplex and it is defined by
$g(\theta)=\frac{\Gamma\left(\sum_{i=1}^{l} a_{i}\right)}{\Pi_{i=1}^{l} \Gamma\left(a_{i}\right)} \prod_{i=1}^{l} \theta_{i}^{a_{i}-1} I(\theta \in \Theta)$
We write $\theta \sim \operatorname{Dirichlet}(a) .$
Note: $E(\theta)=\frac{\left(a_{1}, \ldots, a_{l}\right)}{\sum_{i=1}^{l} a_{i}}=\frac{a}{\dot{a}}=\left(e_{1}, \ldots, e_{l}\right)$
$\operatorname{cov}(\theta)=(\dot{a}+1)^{-1}\left(\begin{array}{cccc}e_{1}\left(1-e_{1}\right) & -e_{1} e_{2} & \ldots & -e_{1} e_{l} \\ -e_{2} e_{1} & e_{2}\left(1-e_{2}\right) & \cdots & -e_{2} e_{l} \\ \vdots & \ddots & \vdots \\ -e_{l} e_{1} & -e_{l} e_{2} & \ldots & e_{l}\left(1-e_{l}\right)\end{array}\right)$

<u>Likelihood</u>: $L(\theta)=\theta_{1}^{x_{1}} \cdots \theta_{l}^{x_{l}}$

<u>Posterior</u>: $g(\theta | x) \propto L(\theta) g(\theta)=\prod_{i=1}^{l} \theta_{i}^{x_{i}+a_{i}-1} I(\theta \in \Theta)$

Thus, we see that $\theta | x \sim \operatorname{Dirichlet}(x+a)$ and $E(\theta | x)=\frac{x+a}{\dot{x}+\dot{a}}$

<u>Suppose we want to predict</u> $Y=\left(Y_{1}, \ldots, Y_{l}\right),$ where $Y | \theta \sim \operatorname{Dirichlet}(m, \theta) .$ We have
$f(y | \theta)=\left(\begin{array}{c}m \\ y_{1}, \ldots, y_{l}\end{array}\right) \theta_{1}^{y_{1}} \cdots \theta_{l}^{y_{l}}$
$g(\theta | x)=\frac{\Gamma(\dot{x}+\dot{a})}{\prod_{i=1}^{l} \Gamma\left(x_{i}+a_{i}\right)} \prod_{i=1}^{l} \theta_{i}^{x_{i}+a_{i}-1} I(\theta \in \Theta)$
since $\int_{\theta \in \Theta} \prod_{i=1}^{l} \theta_{i}^{b_{i}-1} d \theta=\frac{\prod_{i=1}^{l} \Gamma\left(b_{i}\right)}{\Gamma(\dot{b})},$ we have
$f\left(y_{1}, \ldots, y_{k} | x\right)=\int_{\theta \in \Theta} f(y | \theta) g(\theta | x) d \theta=\left(\begin{array}{c}m \\ y_{1}, \ldots, y_{l}\end{array}\right) \frac{\Gamma(\dot{x}+\dot{a})}{\Pi_{i=1}^{l} \Gamma\left(x_{i}+a_{i}\right)} \frac{\Pi_{i=1}^{l} \Gamma\left(x_{i}+a_{i}+y_{i}\right)}{\Gamma(\dot{x}+\dot{a}+\dot{y})}$

$E(Y | X=x)=E[E(Y | \theta) | X=x]=E(m \theta | X=x)=m \frac{x+a}{\dot{x}+\dot{a}}$

By the law of total variance,
$\operatorname{var}\left(Y_{i} | X=x\right)=E\left[\operatorname{var}\left(Y_{i} | \theta\right) | X=x\right]+\operatorname{var}\left[E\left(Y_{i} | \theta\right) | X=x\right]$
$=E\left[m \theta_{i}\left(1-\theta_{i}\right) | X=x\right]+\operatorname{var}\left(m \theta_{i} | X=x\right)$ $=m\left[\frac{x_{i}+a_{i}}{\dot{x}+\dot{a}}-\frac{\left(x_{i}+a_{i}\right)\left(\dot{x}+\dot{a}-x_{i}-a_{i}\right)}{(\dot{x}+\dot{a})^{2}(\dot{x}+\dot{a}+1)}-\left(\frac{x_{i}+a_{i}}{\dot{x}+\dot{a}}\right)^{2}\right]+m^{2} \frac{\left(x_{i}+a_{i}\right)\left(\dot{x}+\dot{a}-x_{i}-a_{i}\right)}{(\dot{x}+\dot{a})^{2}(\dot{x}+\dot{a}+1)}$
$=\frac{\left(x_{i}+a_{i}\right)\left(\dot{x}+\dot{a}-x_{i}-a_{i}\right) m(\dot{x}+\dot{a}+m)}{(\dot{x}+\dot{a})^{2}(\dot{x}+\dot{a}+1)}$



**Multivariate Normal-Inverse Wishart Model**

**Motivating problem**:Suppose we intend to study the joint distribution of the weight $X_{1}$ and height $X_{2}$ and other characteristics $X_{3}, \ldots, X_{d}$ of the group of natives of a certain island aged 12 years old or older. We collect a random sample of size $n$ of $X=\left(X_{1}, X_{2}, \ldots, X_{d}\right)^{T},$ denoted by $X^{1}, \ldots, X^{n} .$

<u>Sampling model</u>: $X^{i} | \theta \sim \operatorname{Normal}(\mu, \Sigma), i=1, \ldots, n,$ where $\theta=(\mu, \Sigma)$
$\mu=\left(\mu_{1}, \ldots, \mu_{d}\right)^{T}$ is the mean vector, and
$\Sigma=\left(\begin{array}{ccc}\Sigma_{11} & \cdots & \Sigma_{1 d} \\ \vdots & \ddots & \vdots \\ \Sigma_{d 1} & \cdots & \Sigma_{d d}\end{array}\right)$
is the positive definite covariance matrix.

<u>Likelihood</u>:
$L(\theta)=|\Sigma|^{-n / 2} \exp \left[-\frac{1}{2} \sum_{i=1}^{n}\left(x^{i}-\bar{x}+\bar{x}-\mu\right)^{T} \Sigma^{-1}\left(x^{i}-\bar{x}+\bar{x}-\mu\right)\right]$
$=|\Sigma|^{-n / 2} \exp \left[-\frac{1}{2} \sum_{i=1}^{n}\left(x^{i}-\bar{x}\right)^{T} \Sigma^{-1}\left(x^{i}-\bar{x}\right)-\frac{1}{2} n(\bar{x}-\mu)^{T} \Sigma^{-1}(\bar{x}-\mu)\right]$
$=|\Sigma|^{-n / 2} \exp \left\{-\frac{1}{2} \sum_{i=1}^{n} \operatorname{trace}\left[\left(x^{i}-\bar{x}\right)^{T} \Sigma^{-1}\left(x^{i}-\bar{x}\right)\right]-\frac{1}{2} n(\bar{x}-\mu)^{T} \Sigma^{-1}(\bar{x}-\mu)\right\}$
$=|\Sigma|^{-n / 2} \exp \left\{-\frac{1}{2} \sum_{i=1}^{n} \operatorname{trace}\left[\Sigma^{-1}\left(x^{i}-\bar{x}\right)\left(x^{i}-\bar{x}\right)^{T}\right]-\frac{1}{2} n(\bar{x}-\mu)^{T} \Sigma^{-1}(\bar{x}-\mu)\right\}$
$=|\Sigma|^{-n / 2} \exp \left\{-\frac{1}{2} \operatorname{trace}\left[\Sigma^{-1} \sum_{i=1}^{n}\left(x^{i}-\bar{x}\right)\left(x^{i}-\bar{x}\right)^{T}\right]-\frac{1}{2} n(\bar{x}-\mu)^{T} \Sigma^{-1}(\bar{x}-\mu)\right\}$
$=|\Sigma|^{-n / 2} \exp \left[-\frac{1}{2} \operatorname{trace}\left(\Sigma^{-1} S\right)-\frac{1}{2} n(\bar{x}-\mu)^{T} \Sigma^{-1}(\bar{x}-\mu)\right]$
where $S=\sum_{i=1}^{n}\left(x^{i}-\bar{x}\right)\left(x^{i}-\bar{x}\right)^{T}$

**Inverse Wishart distribution**: $g(\Sigma)=\frac{|\Psi|^{\frac{v}{2}}}{2^{\frac{v d}{2} \Gamma_{d}\left(\frac{v}{2}\right)}}|\Sigma|^{-\frac{v+d+1}{2}} \exp \left[-\frac{1}{2} \operatorname{trace}\left(\Psi \Sigma^{-1}\right)\right] I(\Sigma$ is psd)
$\Gamma_{d}(a)=\pi^{d(d-1) / 4} \prod_{i=1}^{d} \Gamma[a+(1-i) / 2], v>d-1$
We write $\Sigma \sim \operatorname{InvWishart}(\Psi, v)$
Mean: $E(\Sigma)=\frac{\Psi}{v-d-1}, v>d+1$
Variance: $\operatorname{Var}\left(\Sigma_{i j}\right)=\frac{(v-d+1) \Psi_{i j}^{2}+(v-d-1) \Psi_{i i} \Psi_{j j}}{(v-d)(v-d-1)^{2}(v-d-3)}$

<u>Normal-Inverse Wishart prior</u>:
$\mu | \Sigma \sim \operatorname{Normal}\left(\mu_{0}, \Sigma / n_{0}\right), \Sigma \sim \operatorname{InvWishart}(\Psi, v)$
$g(\mu, \Sigma) \propto|\Sigma|^{-\frac{v+d+1}{2}} \exp \left[-\frac{1}{2} \operatorname{trace}\left(\Sigma^{-1} \Psi\right)\right]|\Sigma|^{-\frac{1}{2}} \exp \left[-\frac{1}{2} n_{0}\left(\mu-\mu_{0}\right)^{T} \Sigma^{-1}\left(\mu-\mu_{0}\right)\right]$
<u>Posterior</u>:
$g(\mu, \Sigma | x) \propto|\Sigma|^{-\frac{n+v+d+1}{2}} \exp \left[-\frac{1}{2} \operatorname{trace}\left(\Sigma^{-1}(\Psi+S)\right)\right]$
$|\Sigma|^{-\frac{1}{2}} \exp \left[-\frac{1}{2} n_{0}\left(\mu-\mu_{0}\right)^{T} \Sigma^{-1}\left(\mu-\mu_{0}\right)-\frac{1}{2} n(\mu-\bar{x})^{T} \Sigma^{-1}(\mu-\bar{x})\right]$
$\propto|\Sigma|^{-\frac{v_{n}+d+1}{2}} \exp \left[-\frac{1}{2} \operatorname{trace}\left(\Sigma^{-1} \Psi_{n}\right)\right]|\Sigma|^{-\frac{1}{2}} \exp \left[-\frac{1}{2}\left(\mu-\mu_{n}\right)^{T} \Sigma_{n}^{-1}\left(\mu-\mu_{n}\right)\right]$
where $v_{n}=n+v, \Sigma_{n}=\left(n_{0}+n\right)^{-1} \Sigma, \mu_{n}=\left(n_{0}+n\right)^{-1}\left(n_{0} \mu_{0}+n \bar{x}\right)$
$\Psi_{n}=\Psi+S+n_{0} \mu_{0} \mu_{0}^{T}+n \bar{x} \bar{x}^{T}-\left(n_{0}+n\right)^{-1}\left(n_{0} \mu_{0}+n \bar{x}\right)\left(n_{0} \mu_{0}+n \bar{x}\right)^{T}=\Psi+S+\left(n_{0}+n\right)^{-1} n_{0} n\left(\bar{x}-\mu_{0}\right)\left(\bar{x}-\mu_{0}\right)^{T}$

<u>Monte Carlo simulation from posterior</u>:
Step 1: Generate \Sigma\simInvWishart(\Psi_n, $v_{n}$ ) (use iwishrnd in Matlab).
Step 2: Generate $\mu \sim$ Normal $\left(\mu_{n}, \Sigma_{n}\right)$ (use munrnd in Matlab).

<u>Monte Carlo simulation from posterior predictive distribution</u>:
In addition to Step 1 and Step 2 above:
Step 3: Generate $Y \sim$ Normal $(\mu, \Sigma)$

