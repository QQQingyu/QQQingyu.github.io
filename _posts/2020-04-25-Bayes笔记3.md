---
layout:     post                    # 使用的布局（不需要改）
title:      Bayes笔记三(Bayes paradigm、Parametric inference、predictive)               # 标题
subtitle:   Series notes of SDSC6003 course. #副标题
date:       2020-04-25              # 时间
author:     Qingyu                      # 作者
header-img: img/post-bg-unix-linux.jpg  #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Bayes Theory
---

Initial information or a priori information not from the data must be translated into a probability model for $\theta,$ say $g(\theta),$ referred to as the prior distribution.

Assume that the sampling model is the family of parameterized PDF's $\{f(x | \theta): \theta \in \Theta\}$ (for discrete is PMF probability mass function)

**The Bayesian paradigm**

<u>In the Bayesian view,</u> any unknown quantity is uncertain and all uncertainties is described in terms of a probability model. In contrast, <u>in frequentist inference</u>, $\theta$ and $F$ are treated as unknown but fixed quantities.

Bayes theorem is used to combine prior information and information from the data to
<u>yield a posterior probability distribution</u> that contains all the information about $\theta$ or $F$.

If we observe $X=x,$ then the posterior PDF of $\theta$ is $g(\theta | x)=f(x | \theta) g(\theta) / \int_{\Theta} f(x | \theta) g(\theta) d \theta$ 

All Bayesian inference is based on $g(\theta | x)$, which contains all the information (from prior + sample) about the parameter $\theta$.

$f(x | \theta)$ can be replaced with $L(\theta)=\kappa f(x | \theta),$ where $\kappa$ is any constant.  The function $L(\theta)$ is called a <u>likelihood function</u> and it is defined only up to a constant of proportionality.

For data $x_{1}, \ldots, x_{n}$ from a random sample, $f(x | \theta)=\prod_{i=1}^{n} h\left(x_{i} | \theta\right),$ and $L(\theta)= \kappa \prod_{i=1}^{n} h\left(x_{i} | \theta\right),$ where $h$ is the marginal PDF or PMF of $X_{i} .$

Suppose $\theta=(\gamma, \phi) \in \Gamma \times \Phi$ and interest centers on inference about $\gamma,$ then we can simply <u>eliminate $\phi$ from the posterior by marginalizing it out</u>. This gives the posterior distribution of $\gamma:$  $g(\gamma | x)=\int_{\Phi} g(\gamma, \phi | x) d \phi=\int_{\Phi} g(\gamma | x, \phi) g(\phi | x) d \phi$
If the prior distribution and likelihood factors into $g(\gamma, \phi)=g(\gamma) g(\phi)$ and $L(\gamma, \phi)=L(\gamma) L(\phi),$ then $\gamma$ and $\phi$ are a <u>posteriori independent,</u> i.e., $g(\gamma, \phi | x)=g(\gamma | x) g(\phi | x)$

**Sufficiency**

Suppose the sampling model can be factored as $f(x | \theta)=h(x) e(t(x) | \theta)$. Then, $t(x)$ is called a <u>sufficient statistic</u> for $\theta$. Moreover, the likelihood function can be taken as $L(\theta)=e(t(x) | \theta)$.  This implies that $g(\theta | x)$ depends on $x$ only through $t(x),$ i.e., $t(x)$ contains all the information in the data about $\theta .$

- Example: Suppose that $X$ has a binomial distribution with $n$ trials and probability of success $\theta$. Then, $t(x)=\bar{x}=\sum_{i=1}^{n} x_{i} / n$ is a sufficient statistic.

- Example 2: Suppose that $X_{1}, \ldots, X_{n}$ are iid gamma random variables, i.e.,
  the sampling model is $f(x | \gamma, \phi)=\prod_{i=1}^{n} \frac{1}{\Gamma(\gamma) \phi^{\gamma}} x_{i}^{\gamma-1} e^{-x_{i} / \phi}=\frac{1}{\Gamma(\gamma) \phi^{\gamma}}(\tilde{x})^{n(\gamma-1)} e^{-n \bar{x} / \phi}$
  where $\tilde{x}=\left(\prod_{i=1}^{n} x_{i}\right)^{1 / n} .$ Note that $(\tilde{x}, \bar{x})$ is a sufficient statistic for $(\gamma, \phi)$

- Example 3: Suppose that $X_{1}, \ldots, X_{n}$ are iid Poisson random variables with mean $\theta,$ i.e., $f(x | \theta)=\prod_{i=1}^{n} \frac{e^{-\theta} \theta^{x_{i}}}{x_{i} !}=\kappa e^{-n \theta} \theta^{n \bar{x}}$
  Note that $\bar{x}$ is a sufficient statistic for $\theta .$

  Let $g(\theta)=\frac{1}{\Gamma(a) b^{a}} \theta^{a-1} e^{-\theta / b},$ i.e., a gamma distribution with parameters $a$ and $b$. Then, $g(\theta | x) \propto e^{-n} \theta^{n \bar{x}} \theta^{a-1} e^{-\theta / b}=\theta^{n \bar{x}+a-1} e^{-(n+1 / b) \theta}$ 

  We see that $g(\theta | x)$ is a gamma distribution with parameters $n \bar{x}+a$ and $n+1 / b$.

- Note: there can be infinitely many sufficient statistics. It is usually desirable to find a sufficient statistic of lower dimension than the data to summarize the data.



**Parametric inference**

**Point estimation**

Classical statisticians: estimate the parameter $\theta$ by a function of the data $T(X),$ called an estimator, and investigate the sampling properties of the estimator such as consistency, unbiasedness etc.

A Bayesian statistician: estimate the parameter $\theta$ as <u>a summary $\hat{\theta}$ of the posterior $g(\theta | x)$ such as its mean, median, or mode</u>. This summary is justified by the use of a loss $\mathcal{L}$ function such that the <u>summary minimizes the expectation of the loss function conditional on the data</u>, called **the risk** $R(\hat{\theta})=E(\mathcal{L}(\hat{\theta}, \theta) | x)$. E.g., the posterior mean of $\theta$ minimizes $R(\hat{\theta})$ when $\mathcal{L}(\hat{\theta}, \theta)=(\hat{\theta}-\theta)^{2}$.

Note that an estimator $\hat{\theta}$ that minimizes $E[\mathcal{L}(\theta, \hat{\theta}) | x]$ also minimizes $E[\mathcal{L}(\theta, \hat{\theta})]$ since $E[\mathcal{L}(\theta, \hat{\theta})]=E\{E[\mathcal{L}(\theta, \hat{\theta}) | x]\}$

- It is common to use the posterior mean as point estimate of $\theta .$ The posterior mean is $\bar{\theta}=\int_{\mathbb{R}^{d}} \theta g(\theta | x) d \theta=\frac{\int_{\mathbb{R}^{d}} \theta L(\theta) g(\theta) d \theta}{\int_{\mathbb{R}^{d}} L(\theta) g(\theta) d \theta}$

  **Theorem 2.1**: <u>Posterior mean is optimal with respect to squared $L 2$ norm loss</u>. Suppose the loss function is $\mathcal{L}(\theta, \varphi)=(\varphi-\theta)^{T}(\varphi-\theta) .$ Then, $\varphi=\bar{\theta}$ minimizes the risk $R(\varphi | x)=E[\mathcal{L}(\theta, \varphi) | x]$ over all possible choices of $\varphi=\psi(x) .$

- **Theorem 2.2**: <u>Posterior median is optimal with respect to $L 1$ norm loss</u>. Suppose $\theta \in \mathbb{R}$ and the loss function is $\mathcal{L}(\theta, \varphi)=|\varphi-\theta| .$ The posterior median of $\theta$ minimizes the risk $R(\varphi | x)=E[\mathcal{L}(\theta, \varphi) | x]$ over all possible choices of $\varphi=\psi(x) .$

- Example: Let $X_{1}, \ldots, X_{n} \sim$ Bernoulli $(\theta) .$ Write $\dot{x}=\sum_{i=1}^{n} x_{i} .$ Thus, $L(\theta)= \theta^{\dot{x}}(1-\theta)^{n-\dot{x}} .$ Let the prior for $\theta$ be $g(\theta)=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \theta^{a-1}(1-\theta)^{b-1}$ i.e., a Beta distribution with parameters $a>0$ and $b>0,$ which <u>we denote as Beta $(a, b) .$ The prior mean of $\theta$ i</u>s $a /(a+b) .$
  Then, the posterior distribution of $p$ is $g(\theta | x) \propto \theta^{\dot{x}+a-1}(1-\theta)^{n-\dot{x}+b-1},$ which is Beta $(\dot{x}+a, n-\dot{x}+b) .$ Thus, $\bar{\theta}=\frac{\dot{x}+a}{n+a+b}$

**Interval Estimation**

Under a Bayesian approach, <u>confidence intervals are replaced by</u> **credible intervals**. For a real parameter $\theta,$ a $1-\alpha$ credible interval is formed by two values $\underline{\theta}$ and $\bar{\theta}$ such that $P(\underline{\theta} \leq \theta \leq \bar{\theta})=\int_{\underline\theta}^{\bar{\theta}} g(\theta | x) d \theta=1-\alpha$ (3)

There are many values for $\theta$ and $\bar{\theta}$ that satisfy (3). A <u>straightforward choice</u> is the equal tail probability credible interval $\int_{-\infty}^{\underline{\theta}} g(\theta | x) d \theta=\int_{\bar{\theta}}^{\infty} g(\theta | x) d \theta=\alpha / 2$

<u>For unimodal posterior densities</u>, a better choice is <u>the shortest interval</u> such that (3) holds. This interval is given by $I=\{\theta: g(\theta | x) \geq \lambda(\alpha)\}$ for some $\lambda(\alpha) .$ Thus, the shortest credible interval is a member of the set of highest posterior density credible sets.

<u>When $g\left(\theta_{i} | x\right)$ is multimodal or skewed</u>.It is the set $C_{\lambda}=\left\{\theta_{i}: g\left(\theta_{i} | x^{n}\right) \geq \lambda\right\}$ that satisfies $\int_{C_{\lambda}} g\left(\theta_{i} | x^{n}\right) d \theta_{i}=1-\alpha$.

**Hypothesis testing** 

It is straightforward to use the Bayesian approach to <u>test the hypothesis</u> $H_{0}: \theta \in \Theta_{0}$ versus $H_{1}: \theta \in \Theta_{1},$ where $\Theta_{0} \cap \Theta_{1}=\emptyset$ and $\Theta_{0} \cup \Theta_{1}=\Theta,$ where $\Theta$ is the sample space of $\theta .$

The probability that $H_{0}$ is true is $P\left(\theta \in \Theta_{0} | x\right)$ while the probability that $H_{1}$ is true is $P\left(\theta \in \Theta_{1} | x\right)=1-P\left(\theta \in \Theta_{0} | x\right) .$

The <u>prior odds</u> for $H_{0}$ versus $H_{1}$ is $P\left(\theta \in \Theta_{0}\right) / P\left(\theta \in \Theta_{1}\right)$ while the <u>posterior odds</u> for $H_{0}$ versus $H_{1}$ is $P\left(\theta \in \Theta_{0} | x\right) / P\left(\theta \in \Theta_{1} | x\right)$

The ratio of the posterior odds to the prior odds is the <u>Bayes factor in favor of</u> $H_{0}$ $B(x)=\frac{P\left(\theta \in \Theta_{0} | x\right) / P\left(\theta \in \Theta_{1} | x\right)}{P\left(\theta \in \Theta_{0}\right) / P\left(\theta \in \Theta_{1}\right)}$

It is also straightforward to use the Bayesian approach <u>for model selection</u>. In this context, one wants to determine which of $K$ models, indexed by $1, \ldots, K,$ that implicitly or explicitly specify the sampling PDF/PME, i.e., $f_{1}\left(x | \theta^{1}\right), \ldots, f_{K}\left(x | \theta^{K}\right),$ is most probable given the data. One can write the sampling model as $f\left(x | \theta^{M}, M\right),$ where $M$ is the model index, and compute the posterior probabilities $P(M=i | x), i=1, \ldots, K .$



**Predictive inference**

In many instances, our interest centers on predicting a set of unobserved $Y_{1}, \ldots, Y_{k}$ random variables given the data $X_{1}= x_{1}, \ldots, X_{n}=x_{n} .$

Let $Y=\left(Y_{1}, \ldots, Y_{k}\right)$ and $X=\left(X_{1}, \ldots, X_{n}\right) .$ From a Bayesianpers pective, we should predict based on the predictive distribution: $f(y | x)=\int_{\Theta} f(y | x, \theta) g(\theta | x) d \theta=\int_{\Theta} \frac{f(y, x | \theta)}{f(x | \theta)} g(\theta | x) d \theta$
where $f(y, x | \theta)$ is the joint sampling model of $(Y, X) .$

Usually, $f(y | x, \theta)=f(y | \theta)$ and $f(y | \theta)$ is specified by the model.



